{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab12_nlp-introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 12 - Natural Language Processing - Introduction\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---------------\n",
        "In this class, we take our first steps into Natural Language Processing (NLP). We'll begin by averaging word embeddings to form sentence-level representations—a simple but effective baseline. **Attention** generalizes this idea by learning which words matter more in context, assigning dynamic weights instead of treating each word equally. In this sense, **attention can be thought of as a learned, weighted average**.\n",
        "\n",
        "This is our gateway into more advanced techniques. In the next class, we’ll study **self-attention**, the backbone of modern architectures like the Transformer. And if time permits, we may even explore the **full Transformer** model in our final class.\n",
        "\n",
        "I would like to express my gratitude to my colleague Przemysław Olbratowski for this elegant way of introducing attention, which I find both intuitive and pedagogically effective.\n"
      ],
      "metadata": {
        "id": "kzosqJ1czsY9"
      },
      "id": "kzosqJ1czsY9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yelp Reviews Polarity Dataset  \n",
        "--------------\n",
        "\n",
        "This dataset contains **over 560k full-text reviews** from Yelp, labeled for **binary sentiment**:  \n",
        "- **positive** (5-star reviews)  \n",
        "- **negative** (1-star reviews)  \n",
        "\n",
        "We will not use the full dataset, because it cannot be handled by Colab RAM. We will use aproximately 20% of total data available.\n",
        "\n",
        "Each example is a **real user-generated review**, typically 2–5 sentences long, capturing clear and direct sentiment in natural language.  \n",
        "There are no ambiguous or neutral labels, making this dataset ideal for training and evaluating **binary sentiment classifiers**.\n",
        "\n",
        "The dataset was curated and released as part of the **FastText** and **Text Classification Benchmarks** by researchers at Facebook AI. It is widely used for benchmarking sentiment models in both academia and industry.\n"
      ],
      "metadata": {
        "id": "G7rPtPD1zvCV"
      },
      "id": "G7rPtPD1zvCV"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVUhR__7NByz",
        "outputId": "b5e0a0d8-d02c-46a4-ecbd-aff769efd426"
      },
      "id": "yVUhR__7NByz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import collections\n",
        "\n",
        "# Load Yelp Polarity dataset from Hugging Face (one-time use)\n",
        "raw = load_dataset(\"yelp_polarity\")\n",
        "\n",
        "N_TRAIN = 100000\n",
        "N_TEST = 20000   # VAL + TEST sets\n",
        "\n",
        "train_raw = raw[\"train\"].select(range(N_TRAIN))\n",
        "test_raw  = raw[\"test\"].select(range(N_TEST))\n",
        "\n",
        "# Extract plain Python lists for text and labels\n",
        "def to_lists(dataset):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for example in dataset:\n",
        "        texts.append(example[\"text\"])\n",
        "        labels.append(example[\"label\"])\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = to_lists(train_raw)\n",
        "test_texts, test_labels   = to_lists(test_raw)\n",
        "\n",
        "# Split test set into val/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "val_texts, final_test_texts, val_labels, final_test_labels = train_test_split(\n",
        "    test_texts, test_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Show class counts\n",
        "def print_distribution(name, labels):\n",
        "    c = collections.Counter(labels)\n",
        "    total = sum(c.values())\n",
        "    print(f\"\\n{name} distribution:\")\n",
        "    for label in sorted(c.keys()):\n",
        "        print(f\"  {label}: {c[label]} ({c[label]/total:.2%})\")\n",
        "\n",
        "print_distribution(\"Train\", train_labels)\n",
        "print_distribution(\"Validation\", val_labels)\n",
        "print_distribution(\"Test\", final_test_labels)\n"
      ],
      "metadata": {
        "id": "E8GmG8SAzsE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd08cec-934d-41e6-cb6b-0d817e50a6f1"
      },
      "id": "E8GmG8SAzsE6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some examples"
      ],
      "metadata": {
        "id": "oH3nWRZnNAxB"
      },
      "id": "oH3nWRZnNAxB"
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"[{label_map[train_labels[i]]}] {train_texts[i]}\\n\")"
      ],
      "metadata": {
        "id": "2swC8sM0ND49"
      },
      "id": "2swC8sM0ND49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "-------------------\n",
        "\n",
        "To feed text into a neural network, we need to represent words in a \"neural-network-ish\" way — that is, as numbers. The standard approach is to use a tokenizer, often from a pretrained model. However, since we plan to experiment with our own attention modules later on, **we’ll avoid using any pretrained tokenizer**.\n",
        "\n",
        "Instead, we’ll go with a simple, word-based tokenization. As part of this, we’ll clean the text by removing any non-standard HTML tags, digits, extra whitespace, and punctuation. We’ll also convert all words to lowercase to ensure consistency."
      ],
      "metadata": {
        "id": "T07SvDTYV6nO"
      },
      "id": "T07SvDTYV6nO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Tokens: `<PAD>` and `<UNK>`\n",
        "\n",
        "In our text preprocessing pipeline, we convert each word to a number using a vocabulary. Two special tokens help us handle padding and unknown words.\n"
      ],
      "metadata": {
        "id": "IyT2bcscZu4P"
      },
      "id": "IyT2bcscZu4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### `<PAD>` — Padding Token\n",
        "\n",
        "- Represents empty slots when we need all input sequences to be the same length.\n",
        "- Assigned index `0`.\n",
        "- Used so that batches of sentences can be processed together by the model.\n",
        "\n",
        "*For example:*\n",
        "\n",
        "Original: `[17, 5, 23]`  \n",
        "Padded:   `[17, 5, 23, 0, 0]` (for a fixed length of 5)"
      ],
      "metadata": {
        "id": "Ha9i37gvaBUn"
      },
      "id": "Ha9i37gvaBUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### `<UNK>` — Unknown Token\n",
        "\n",
        "- Represents any word that is **not in the vocabulary**.\n",
        "- Assigned index `1`.\n",
        "- Occurs when:\n",
        "  1. A word was **too rare in the training data** (appeared only once and was excluded from the vocabulary).\n",
        "  2. A word appears **only in validation or test data**.\n",
        "\n",
        "> In our setup, we **excluded all words that appear only once** in the training set.  \n",
        "> So even in the training data, some tokens are replaced with `<UNK>`.  \n",
        "> These are called **rare unknowns** — they help the model learn how to handle unusual or unfamiliar words.\n",
        "\n",
        "\n",
        "By including `<UNK>` during training, we teach the model how to deal with unseen or rare words at test time — which is **crucial for generalization**.\n"
      ],
      "metadata": {
        "id": "3Rymjwd6ZsoK"
      },
      "id": "3Rymjwd6ZsoK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re                             # Regular Expressions: for text cleaning\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.split()\n",
        "\n",
        "def build_vocab(token_lists, min_freq=2):\n",
        "    counter = collections.Counter(token for tokens in token_lists for token in tokens)\n",
        "    vocab = {\n",
        "        token: idx + 2  # reserve 0: <PAD>, 1: <UNK>\n",
        "        for idx, (token, count) in enumerate(counter.items())\n",
        "        if count >= min_freq\n",
        "    }\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "def tokens_to_ids(tokens, vocab):\n",
        "    return [vocab.get(tok, vocab['<UNK>']) for tok in tokens]   #for unknown tokens return vocab['<UNK>'] (which is == 1)\n",
        "\n",
        "def pad(seq, max_len=128, pad_value=0):\n",
        "    return seq + [pad_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "\n",
        "def process_texts(texts, vocab, max_len=128):\n",
        "    return [pad(tokens_to_ids(tokenize(text), vocab), max_len) for text in texts]\n",
        "\n",
        "\n",
        "# Tokenize training set and build vocab\n",
        "train_tokens = [tokenize(t) for t in train_texts]\n",
        "vocab = build_vocab(train_tokens)\n",
        "\n",
        "# Process splits into padded input_ids\n",
        "train_ids = process_texts(train_texts, vocab, MAX_LEN)\n",
        "val_ids   = process_texts(val_texts, vocab, MAX_LEN)\n",
        "test_ids  = process_texts(test_texts, vocab, MAX_LEN)\n",
        "\n",
        "# Print 10 real examples: raw text, tokenized, and input IDs\n",
        "print(\"\\n🔍 Sample Yelp reviews (original + tokenized + input_ids):\\n\")\n",
        "for i in range(10):\n",
        "    print(f\"Original:   {train_texts[i]}\")\n",
        "    print(f\"Tokenized:  {train_tokens[i]}\")\n",
        "    print(f\"Input IDs:  {train_ids[i]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bNJLBhifYFhO"
      },
      "id": "bNJLBhifYFhO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notice!\n",
        "\n",
        "Notice that the `<UNK>` token (coded as 1) is visible in the rows above. Also, there is an abundance of `<PAD>` tokens (coded as 0)."
      ],
      "metadata": {
        "id": "cSDc_W9S89ZU"
      },
      "id": "cSDc_W9S89ZU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data Loaders\n",
        "--------------------\n",
        "\n",
        "We must transform the pandas dataframe to the dataset - it will, among other things, separate input data and labels and then wrap it in a dataloder."
      ],
      "metadata": {
        "id": "XDLaOuwegNXu"
      },
      "id": "XDLaOuwegNXu"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SoiJPpApJe9L"
      },
      "id": "SoiJPpApJe9L"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Convert all to tensors\n",
        "def to_loader(input_ids, labels, batch_size=1024, shuffle=False):\n",
        "    x_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
        "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_tensor, y_tensor), batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = to_loader(train_ids, train_labels, BATCH_SIZE, shuffle=True)\n",
        "val_loader   = to_loader(val_ids, val_labels, BATCH_SIZE)\n",
        "test_loader  = to_loader(test_ids, test_labels, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "432ev7Jxg9Fw"
      },
      "id": "432ev7Jxg9Fw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Neural Network\n",
        "----------------------"
      ],
      "metadata": {
        "id": "cOF7Pkk5hdTd"
      },
      "id": "cOF7Pkk5hdTd"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_16(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #batch, words\n",
        "        features = self.embedding(index)             #batch, words, features\n",
        "        features = features.mean(-2)                 #batch, features\n",
        "        classifications = self.classifier(features)  #batch, 1\n",
        "        logits = classifications.squeeze(-1)         #batch\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "obxbQeichhV-"
      },
      "id": "obxbQeichhV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop as We Already Got to Know It Well\n",
        "----------------"
      ],
      "metadata": {
        "id": "bOD1hweknD7z"
      },
      "id": "bOD1hweknD7z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop Function"
      ],
      "metadata": {
        "id": "7kZDRRGS-yMI"
      },
      "id": "7kZDRRGS-yMI"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "def train_model(net, train_loader, val_loader, epochs=1000, lr=0.001, log_every=10):\n",
        "    print(f\"Working on {device}\")\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_inputs, batch_labels in train_loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device).float()  # shape: (batch_size)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = net(batch_inputs)  # logits shape: (batch_size, 1)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            train_correct += (preds == batch_labels).sum().item()\n",
        "            train_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss_sum / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # === Validation ===\n",
        "        net.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_labels in val_loader:\n",
        "                batch_inputs = batch_inputs.to(device)\n",
        "                batch_labels = batch_labels.to(device).float()\n",
        "\n",
        "                logits = net(batch_inputs)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "\n",
        "                val_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "                val_correct += (preds == batch_labels).sum().item()\n",
        "                val_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss_sum / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        if epoch % log_every == 0:\n",
        "            print(f\"Epoch {epoch:03d} | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history"
      ],
      "metadata": {
        "id": "HCA6aXtlnH6J"
      },
      "id": "HCA6aXtlnH6J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Function"
      ],
      "metadata": {
        "id": "-P8_EMUj-tnp"
      },
      "id": "-P8_EMUj-tnp"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(train_loss, val_loss, train_acc, val_acc):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    axs[0].plot(train_loss, label=\"Train Loss\", color='blue')\n",
        "    axs[0].plot(val_loss, label=\"Val Loss\", color='orange')\n",
        "    axs[0].set_title(\"Loss per Epoch\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Average Loss\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    axs[1].plot(train_acc, label=\"Train Accuracy\", color='green')\n",
        "    axs[1].plot(val_acc, label=\"Val Accuracy\", color='red')\n",
        "    axs[1].set_title(\"Accuracy per Epoch\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mDuVP9J6zl0T"
      },
      "id": "mDuVP9J6zl0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Training"
      ],
      "metadata": {
        "id": "fTOMeRt_-2Th"
      },
      "id": "fTOMeRt_-2Th"
    },
    {
      "cell_type": "code",
      "source": [
        "net_16=Net_16()\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_16, train_loader, val_loader, epochs=200)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "krgbt41P-5Aw"
      },
      "id": "krgbt41P-5Aw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Aproach\n",
        "\n",
        "It doesn't improve beyond 90% accuract. So maybe we can go simpler, then."
      ],
      "metadata": {
        "id": "_GUYSKp3HlRK"
      },
      "id": "_GUYSKp3HlRK"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 1\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #batch, words\n",
        "        features = self.embedding(index)             #batch, words, features\n",
        "        features = features.mean(-2)                 #batch, features\n",
        "        classifications = self.classifier(features)  #batch, 1\n",
        "        logits = classifications.squeeze(-1)         #batch\n",
        "        return logits\n",
        "\n",
        "net_1 = Net_1()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_1, train_loader, val_loader, epochs = 200)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "mNwmWLEuHrOD"
      },
      "id": "mNwmWLEuHrOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Polarity\n",
        "\n",
        "Since our embeddings have only 1 feature (`feature_cnt = 1`), each word is embedded to a scalar. We can interpret this scalar as a kind of sentiment polarity, especially since our model is trained for sentiment classification."
      ],
      "metadata": {
        "id": "mlg7DIdoXA_Y"
      },
      "id": "mlg7DIdoXA_Y"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the embedding weights as a NumPy array\n",
        "embedding_weights = net_1.embedding.weight.detach().cpu().numpy().squeeze()  # shape: (vocab_len,)\n",
        "\n",
        "# Reverse vocab dictionary to map indices back to words\n",
        "id2token = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# Skip <PAD> and <UNK> tokens (indices 0 and 1)\n",
        "valid_indices = np.array([idx for idx in range(2, len(embedding_weights)) if idx in id2token])\n",
        "valid_embeddings = embedding_weights[valid_indices]\n",
        "\n",
        "# Sort and select indices\n",
        "sorted_pos = np.argsort(-valid_embeddings)\n",
        "sorted_neg = np.argsort(valid_embeddings)\n",
        "sorted_neutral = np.argsort(np.abs(valid_embeddings))\n",
        "\n",
        "top_pos_indices = valid_indices[sorted_pos[:20]]\n",
        "top_neg_indices = valid_indices[sorted_neg[:20]]\n",
        "top_neutral_indices = valid_indices[sorted_neutral[:20]]\n",
        "\n",
        "# Print words and corresponding embedding values\n",
        "def print_words_with_embeddings(indices, title):\n",
        "    print(f\"\\n{title}\")\n",
        "    for idx in indices:\n",
        "        word = id2token[int(idx)]\n",
        "        value = embedding_weights[int(idx)]\n",
        "        print(f\"{word:15} -> {value:.4f}\")\n",
        "\n",
        "print_words_with_embeddings(top_pos_indices, \"Top 20 most positive words:\")\n",
        "print_words_with_embeddings(top_neg_indices, \"Top 20 most negative words:\")\n",
        "print_words_with_embeddings(top_neutral_indices, \"Top 20 most neutral words:\")\n"
      ],
      "metadata": {
        "id": "zPijTrZEXvM4"
      },
      "id": "zPijTrZEXvM4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificator\n",
        "\n",
        "It is interesting to see how the polar sentiment gets translated into the two  class values. Let's see:"
      ],
      "metadata": {
        "id": "ayZtmbdcvPyP"
      },
      "id": "ayZtmbdcvPyP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract weights and bias from the classifier layer\n",
        "classifier_weight = net_1.classifier.weight.detach().cpu().numpy()\n",
        "classifier_bias = net_1.classifier.bias.detach().cpu().numpy()\n",
        "\n",
        "print(\"Classifier weights (shape: {}):\".format(classifier_weight.shape))\n",
        "print(classifier_weight)\n",
        "\n",
        "print(\"\\nClassifier bias (shape: {}):\".format(classifier_bias.shape))\n",
        "print(classifier_bias)\n",
        "\n",
        "print(\"Recall our coding: \")\n",
        "print(label_map)\n"
      ],
      "metadata": {
        "id": "0Ra3BaKGv7ib"
      },
      "id": "0Ra3BaKGv7ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand-picked test sentences\n",
        "texts = [\n",
        "    \"I love this!\",\n",
        "    \"This is terrible.\",\n",
        "    \"Thank you so much!\",\n",
        "    \"I hate this.\",\n",
        "    \"Wow, awesome!\",\n",
        "    \"This sucks.\"\n",
        "]\n",
        "\n",
        "# Convert texts to input_ids using your tokenizer\n",
        "input_ids = process_texts(texts, vocab)\n",
        "input_tensor = torch.tensor(input_ids).to(next(net_1.parameters()).device)\n",
        "\n",
        "# Predict with trained model\n",
        "net_1.eval()\n",
        "with torch.no_grad():\n",
        "    logits = net_1(input_tensor).squeeze()\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Print results\n",
        "for text, prob in zip(texts, probs):\n",
        "    print(f\"{text:30} -> predicted probability of POSITIVE: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "00X0GbtFELx7"
      },
      "id": "00X0GbtFELx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Towards Attention!\n",
        "\n"
      ],
      "metadata": {
        "id": "cnwL4ci4aRMK"
      },
      "id": "cnwL4ci4aRMK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Entry Point\n",
        "\n",
        "The entry point is our Net_16 model with averaged features:"
      ],
      "metadata": {
        "id": "ZeX-Tnlra-pY"
      },
      "id": "ZeX-Tnlra-pY"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_16(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #SIZE: batch, words\n",
        "        features = self.embedding(index)             #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where AVERAGING takes place\n",
        "        features = features.mean(-2)                 #SIZE: batch, features\n",
        "\n",
        "\n",
        "        classifications = self.classifier(features)  #SIZE: batch, 1\n",
        "        logits = classifications.squeeze(-1)         #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "diAmuIhIbHOQ"
      },
      "id": "diAmuIhIbHOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Less Explicit Averaging"
      ],
      "metadata": {
        "id": "H-g5Q7KxbWzG"
      },
      "id": "H-g5Q7KxbWzG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is equivalent to taking the `mean()` over words:"
      ],
      "metadata": {
        "id": "KLDhT9JCd9gh"
      },
      "id": "KLDhT9JCd9gh"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        weights = torch.ones((batch, 1, words)) / words   # create EQUAL WEIGHT tensor summing to 1.0 ( words x (1/words) )\n",
        "                                                         #SIZE: batch, 1, words\n",
        "        features = weights @ features                    #SIZE: batch, 1, features\n",
        "\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "epoXI3SZbcHg"
      },
      "id": "epoXI3SZbcHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let us refactor this, taking the weighted average part into a separate `Attention` module:"
      ],
      "metadata": {
        "id": "OFcB29hteVru"
      },
      "id": "OFcB29hteVru"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, features):                         #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        weights = torch.ones((batch, 1, words)) / words   # create EQUAL WEIGHT tensor summing to 1.0 ( words x (1/words) )\n",
        "                                                         #SIZE: batch, 1, words\n",
        "        features = weights @ features                    #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "8FzeUuJ-ehns"
      },
      "id": "8FzeUuJ-ehns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that we did up until that point was to rewrite taking the `mean()` into a separate `Attention` which calculates the weighted averager with equal weights over weights."
      ],
      "metadata": {
        "id": "fKuofPqsfWxe"
      },
      "id": "fKuofPqsfWxe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Notion of Energy\n",
        "\n",
        "Another useful concept is that of the energy. The energy equal to 0 uniformly for all words translates (with the use of `softmax`) into the equal weights, so the below version is still equivalent to what we already had (but, arguably, it looks much more complex):"
      ],
      "metadata": {
        "id": "3cFkehdKf1vE"
      },
      "id": "3cFkehdKf1vE"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = torch.zeros((batch, 1, words))       #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "IVkMyyYjgN6p"
      },
      "id": "IVkMyyYjgN6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Something New - Learned Energies"
      ],
      "metadata": {
        "id": "8Q4efFtch23v"
      },
      "id": "8Q4efFtch23v"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.energy = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = self.energy(features)                #SIZE: batch, words, 1\n",
        "        energies = energies.transpose(-2, -1)           #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with LEARNED WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits\n",
        "\n",
        "net_att = Net_16_Towards_Attention()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att, train_loader, val_loader, epochs = 100)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "8VLpkoPbh9LG"
      },
      "id": "8VLpkoPbh9LG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}