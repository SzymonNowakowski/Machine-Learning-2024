{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab12_nlp-introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 12 - Natural Language Processing - Introduction\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---------------\n",
        "In this class, we take our first steps into Natural Language Processing (NLP). We'll begin by averaging word embeddings to form sentence-level representations—a simple but effective baseline. **Attention** generalizes this idea by learning which words matter more in context, assigning dynamic weights instead of treating each word equally. In this sense, **attention can be thought of as a learned, weighted average**.\n",
        "\n",
        "This is our gateway into more advanced techniques. In the next class, we’ll study **self-attention**, the backbone of modern architectures like the Transformer. And if time permits, we may even explore the **full Transformer** model in our final class.\n",
        "\n",
        "I would like to express my gratitude to my colleague Przemysław Olbratowski for this elegant way of introducing attention, which I find both intuitive and pedagogically effective.\n"
      ],
      "metadata": {
        "id": "kzosqJ1czsY9"
      },
      "id": "kzosqJ1czsY9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yelp Reviews Polarity Dataset  \n",
        "--------------\n",
        "\n",
        "This dataset contains **over 560k full-text reviews** from Yelp, labeled for **binary sentiment**:  \n",
        "- **positive** (5-star reviews)  \n",
        "- **negative** (1-star reviews)  \n",
        "\n",
        "We will not use the full dataset, because it cannot be handled by Colab RAM. We will use aproximately 20% of total data available.\n",
        "\n",
        "Each example is a **real user-generated review**, typically 2–5 sentences long, capturing clear and direct sentiment in natural language.  \n",
        "There are no ambiguous or neutral labels, making this dataset ideal for training and evaluating **binary sentiment classifiers**.\n",
        "\n",
        "The dataset was curated and released as part of the **FastText** and **Text Classification Benchmarks** by researchers at Facebook AI. It is widely used for benchmarking sentiment models in both academia and industry.\n"
      ],
      "metadata": {
        "id": "G7rPtPD1zvCV"
      },
      "id": "G7rPtPD1zvCV"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVUhR__7NByz",
        "outputId": "0715fb5e-7b5b-4159-f3e7-2c77f142b149"
      },
      "id": "yVUhR__7NByz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import collections\n",
        "\n",
        "# Load Yelp Polarity dataset from Hugging Face (one-time use)\n",
        "raw = load_dataset(\"yelp_polarity\")\n",
        "\n",
        "N_TRAIN = 100000\n",
        "N_TEST = 20000   # VAL + TEST sets\n",
        "\n",
        "train_raw = raw[\"train\"].shuffle(seed=42).select(range(N_TRAIN))\n",
        "test_raw  = raw[\"test\"].shuffle(seed=42).select(range(N_TEST))\n",
        "\n",
        "# Extract plain Python lists for text and labels\n",
        "def to_lists(dataset):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for example in dataset:\n",
        "        texts.append(example[\"text\"])\n",
        "        labels.append(example[\"label\"])\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = to_lists(train_raw)\n",
        "test_texts, test_labels   = to_lists(test_raw)\n",
        "\n",
        "# Split test set into val/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    test_texts, test_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Show class counts\n",
        "def print_distribution(name, labels):\n",
        "    c = collections.Counter(labels)\n",
        "    total = sum(c.values())\n",
        "    print(f\"\\n{name} distribution:\")\n",
        "    for label in sorted(c.keys()):\n",
        "        print(f\"  {label}: {c[label]} ({c[label]/total:.2%})\")\n",
        "\n",
        "print_distribution(\"Train\", train_labels)\n",
        "print_distribution(\"Validation\", val_labels)\n",
        "print_distribution(\"Test\", test_labels)\n"
      ],
      "metadata": {
        "id": "E8GmG8SAzsE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c297d8e8-cff8-40a3-8ad3-0139c8768e9b"
      },
      "id": "E8GmG8SAzsE6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train distribution:\n",
            "  0: 50209 (50.21%)\n",
            "  1: 49791 (49.79%)\n",
            "\n",
            "Validation distribution:\n",
            "  0: 5038 (50.38%)\n",
            "  1: 4962 (49.62%)\n",
            "\n",
            "Test distribution:\n",
            "  0: 4973 (49.73%)\n",
            "  1: 5027 (50.27%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some examples"
      ],
      "metadata": {
        "id": "oH3nWRZnNAxB"
      },
      "id": "oH3nWRZnNAxB"
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"[{label_map[train_labels[i]]}] {train_texts[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2swC8sM0ND49",
        "outputId": "f9630581-4945-46c2-8f79-dc8590d6f01c"
      },
      "id": "2swC8sM0ND49",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[positive] Decent size, decent selection, decent staff.\\n\\nI guess that can wholly sum this place up, it's decent.  As with many other stores that are like this, the product rotates depending on what doesn't sale well at other stores.  Can always snag a deal here.  I was able to pick up a pretty sweet Puma jacket for $10, can't beat that, right?\\n\\nThat being said, there are those times that you may not find anything as well.  So really don't get your hopes up if you are looking for a specific item.\n",
            "\n",
            "[negative] I have definitely experienced better! Let's see, so I first brought my car here to get the brakes done and they did a pretty good job, although, every time I'm in reverse, my brakes do make a squeaking noise. Not sure what's up with that. Anyway, needless to say, they did a pretty good job on my brakes. \\n\\nSince my last brake job, they have switched managers and the service has kinda gone down hill. I brought my car in to have the control arm bushings done. They quoted me a little under $500 to have this done, which wasn't really that much cheaper than the dealer. The dealer was about $50 more and would have given me a loaner vehicle to use. \\n\\nAnyway, I brought my car in to have them work on it. I dropped it off at 12:00pm, which was my scheduled appointment time. At 2:30pm I received a call from the manager stating that they ordered the wrong part and couldn't do the work on the car today. He told me I could either pick it up or just leave it there. I'm not sure why it took 2 1/2 hours to figure out that you ordered the wrong part...but I didn't have 2 days to spend on having my vehicle repaired. I mean, my whole day was pretty much wasted. \\n\\nI decided to pick my car up and call around to see if there was another repair shop to do the work I requested. I wound up finding Kelly's Automotive Repair in Fountain Hills to do the work for $357 including parts, labor, shop supplies, etc. Needless to say, because Just Brakes was incompetent in ordering the wrong part...and taking 2 1/2 hours to figure it out, they lost my business. Silver lining is that I saved over $100 by them doing that.\\n\\nI'll probably bring my car back for them to look at why my brakes squeak when I back up, since they were the last ones who did my brakes. Other than that, I don't really trust them anymore.\n",
            "\n",
            "[negative] Food is mediocre at best. I came in one day and ordered the lo mein and they use spaghetti pasta to pass as lo mein noodles.\n",
            "\n",
            "[positive] Best casino in the PHX area.\n",
            "\n",
            "[positive] This place is a must try! Truly a fan of Skinny Fats!\\n\\nThe first time I went was around lunchtime, so I tried the Buff Chix- so yummy! Love the buffalo sauce! \\n\\nThe next time I went was for breakfast, so I decided to try 3 Happy Chicks. Again- so yummy! This is now my go to item all the time! Especially since they serve breakfast all day!\\n\\nI would love to try everything on the menu! But every time I go, I always seem to order the 3 Happy Chicks!  It's a simple breakfast- just made perfectly! \\n\\nThe location is a smaller place, which is always packed, but I think that this is just part of the fun and experience!\\n \\nThe staff is amazing... full of fun, and awesome-ness, and just always in a great mood!\n",
            "\n",
            "[positive] This is just for the food. I wish I could have seen a band here, but was in town for business and just didn't have time. I flew in from KC late Monday night (for my time zone) and missed dinner with my coworkers, so I decided to head out on my own. I was staying at Mandalay Bay, so I didn't have far to go to House of Blues. It's right in that complex.\\n\\nI asked to sit at the bar. That's usually what I do if i am eating alone. You get seated right away, your server is never far away and the service is usually superb. I was not disappointed.\\n\\nI asked the bartender for a light, savory gin drink. I can't remember what he called it, but it hit the spot. Then I asked what was good on the menu. Initially he said, \\\"\"Everything\\\"\". I said BS to that. Then he asked what I wanted - I said something light, it's late for me. So, he recommended the shrimp tacos.\\n\\nThe food came out pretty quickly. The shrimp tacos were really good. Lots of shrimp, nicely seasoned with a little kick. I asked for hot sauce, but all they had was Tabasco. That surprised me. Didn't this chain start in New Orleans? However, they had some sauce that really highlighted the flavor, so kudos.\\n\\nI was out in less than an hour. Nice dinner. I'd go back - next time for a concert, hopefully.\n",
            "\n",
            "[positive] Best fried rice I've tried. My girlfriend ordered the shrimp fried rice and added jalapenos and XO sauce and it was delicious! Its not on the menu but you can ask for it. The beef chow fun was good too but a little plain.\n",
            "\n",
            "[negative] One of the low end Japanese express style of restaurants in the university area. The rice is horrible, the plates are over priced ($2-$3 dollars more than other places),  and the serve is mediocre at best. \\n\\nGiven the option, I'd steer clear of this place and go other \\\"\"quality\\\"\" restaurants in the area. But i'd say try this place out if you enjoy drowning your plates in shrimp sauce; Its all going to taste the same anyways.\n",
            "\n",
            "[negative] The only benefit to this place was that it was a good deal. That being said, it was dirty, DIRTY- like we had to get a new room because we couldn't stand the shower-scum shower, dust coming out of fans and stains on pillows that came from questionable sources. It is off the strip, so you have to take the bus to drop you off, which can lead to a 45 minute wait. The inside needs some serious updating to match the other hotels, looks as if it is stuck in the 80's. Tube TV's in the rooms, no fan in the bathrooms, keurig machines in rooms that don't work..This place needs some serious work. Will NOT be staying here again.\n",
            "\n",
            "[negative] I have never written a review, but the food at Havana Cafe was so underwhelming (and not in the least bit good) that I feel compelled to share. \\nI ordered the Pollo Ajillo which, in reality, was a pile of salt with a side of chicken and rice. I opted for Arroz con Gandules instead of white rice as it has been years since I have had this tasty Puerto Rican side. The whole thing was almost inedible... I came home and actually brushed my teeth immediately, it was so bad!  My husband had the Pollo Cubano, which was equally underwhelming and nothing that we could not make at home ourselves.  The restaurant gets an extra star for the wonderful Cafe Cubano and the extremely attentive wait staff .  I had high hopes for this place but the next time I have a craving for Cuban food, I will be heading to Sabor Cubano.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "-------------------\n",
        "\n",
        "To feed text into a neural network, we need to represent words in a \"neural-network-ish\" way — that is, as numbers. The standard approach is to use a tokenizer, often from a pretrained model. However, since we plan to experiment with our own attention modules later on, **we’ll avoid using any pretrained tokenizer**.\n",
        "\n",
        "Instead, we’ll go with a simple, word-based tokenization. As part of this, we’ll clean the text by removing any non-standard HTML tags, digits, extra whitespace, and punctuation. We’ll also convert all words to lowercase to ensure consistency."
      ],
      "metadata": {
        "id": "T07SvDTYV6nO"
      },
      "id": "T07SvDTYV6nO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Tokens: `<PAD>` and `<UNK>`\n",
        "\n",
        "In our text preprocessing pipeline, we convert each word to a number using a vocabulary. Two special tokens help us handle padding and unknown words.\n"
      ],
      "metadata": {
        "id": "IyT2bcscZu4P"
      },
      "id": "IyT2bcscZu4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### `<PAD>` — Padding Token\n",
        "\n",
        "- Represents empty slots when we need all input sequences to be the same length.\n",
        "- Assigned index `0`.\n",
        "- Used so that batches of sentences can be processed together by the model.\n",
        "\n",
        "*For example:*\n",
        "\n",
        "Original: `[17, 5, 23]`  \n",
        "Padded:   `[17, 5, 23, 0, 0]` (for a fixed length of 5)"
      ],
      "metadata": {
        "id": "Ha9i37gvaBUn"
      },
      "id": "Ha9i37gvaBUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### `<UNK>` — Unknown Token\n",
        "\n",
        "- Represents any word that is **not in the vocabulary**.\n",
        "- Assigned index `1`.\n",
        "- Occurs when:\n",
        "  1. A word was **too rare in the training data** (appeared only once and was excluded from the vocabulary).\n",
        "  2. A word appears **only in validation or test data**.\n",
        "\n",
        "> In our setup, we **excluded all words that appear only once** in the training set.  \n",
        "> So even in the training data, some tokens are replaced with `<UNK>`.  \n",
        "> These are called **rare unknowns** — they help the model learn how to handle unusual or unfamiliar words.\n",
        "\n",
        "\n",
        "By including `<UNK>` during training, we teach the model how to deal with unseen or rare words at test time — which is **crucial for generalization**.\n"
      ],
      "metadata": {
        "id": "3Rymjwd6ZsoK"
      },
      "id": "3Rymjwd6ZsoK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re                             # Regular Expressions: for text cleaning\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.split()\n",
        "\n",
        "def build_vocab(token_lists, min_freq=2):\n",
        "    counter = collections.Counter(token for tokens in token_lists for token in tokens)\n",
        "    vocab = {\n",
        "        token: idx + 2  # reserve 0: <PAD>, 1: <UNK>\n",
        "        for idx, (token, count) in enumerate(counter.items())\n",
        "        if count >= min_freq\n",
        "    }\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "def tokens_to_ids(tokens, vocab):\n",
        "    return [vocab.get(tok, vocab['<UNK>']) for tok in tokens]   #for unknown tokens return vocab['<UNK>'] (which is == 1)\n",
        "\n",
        "def pad(seq, max_len=128, pad_value=0):\n",
        "    return seq + [pad_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "\n",
        "def process_texts(texts, vocab, max_len=128):\n",
        "    return [pad(tokens_to_ids(tokenize(text), vocab), max_len) for text in texts]\n",
        "\n",
        "\n",
        "# Tokenize training set and build vocab\n",
        "train_tokens = [tokenize(t) for t in train_texts]\n",
        "vocab = build_vocab(train_tokens)\n",
        "\n",
        "# Process splits into padded input_ids\n",
        "train_ids = process_texts(train_texts, vocab, MAX_LEN)\n",
        "val_ids   = process_texts(val_texts, vocab, MAX_LEN)\n",
        "test_ids  = process_texts(test_texts, vocab, MAX_LEN)\n",
        "\n",
        "# Print 10 real examples: raw text, tokenized, and input IDs\n",
        "print(\"\\nSample Yelp reviews (original + tokenized + input_ids):\\n\")\n",
        "for i in range(10, 20):\n",
        "    print(f\"Original:   {train_texts[i]}\")\n",
        "    print(f\"Tokenized:  {train_tokens[i]}\")\n",
        "    print(f\"Input IDs:  {train_ids[i]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bNJLBhifYFhO"
      },
      "id": "bNJLBhifYFhO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notice!\n",
        "\n",
        "Notice that the `<UNK>` token (coded as 1) is visible in the rows above. Also, there is an abundance of `<PAD>` tokens (coded as 0)."
      ],
      "metadata": {
        "id": "cSDc_W9S89ZU"
      },
      "id": "cSDc_W9S89ZU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data Loaders\n",
        "--------------------\n",
        "\n",
        "We must transform the pandas dataframe to the dataset - it will, among other things, separate input data and labels and then wrap it in a dataloder."
      ],
      "metadata": {
        "id": "XDLaOuwegNXu"
      },
      "id": "XDLaOuwegNXu"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SoiJPpApJe9L"
      },
      "id": "SoiJPpApJe9L"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Convert all to tensors\n",
        "def to_loader(input_ids, labels, batch_size=1024, shuffle=False):\n",
        "    x_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
        "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_tensor, y_tensor), batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = to_loader(train_ids, train_labels, BATCH_SIZE, shuffle=True)\n",
        "val_loader   = to_loader(val_ids, val_labels, BATCH_SIZE)\n",
        "test_loader  = to_loader(test_ids, test_labels, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "432ev7Jxg9Fw"
      },
      "id": "432ev7Jxg9Fw",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Neural Network\n",
        "----------------------"
      ],
      "metadata": {
        "id": "cOF7Pkk5hdTd"
      },
      "id": "cOF7Pkk5hdTd"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_16(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #batch, words\n",
        "        features = self.embedding(index)             #batch, words, features\n",
        "        features = features.mean(-2)                 #batch, features\n",
        "        classifications = self.classifier(features)  #batch, 1\n",
        "        logits = classifications.squeeze(-1)         #batch\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "obxbQeichhV-"
      },
      "id": "obxbQeichhV-",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop as We Already Got to Know It Well\n",
        "----------------"
      ],
      "metadata": {
        "id": "bOD1hweknD7z"
      },
      "id": "bOD1hweknD7z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop Function"
      ],
      "metadata": {
        "id": "7kZDRRGS-yMI"
      },
      "id": "7kZDRRGS-yMI"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "def train_model(net, train_loader, val_loader, epochs=1000, lr=0.001, log_every=10):\n",
        "    print(f\"Working on {device}\")\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_inputs, batch_labels in train_loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device).float()  # shape: (batch_size)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = net(batch_inputs)  # logits shape: (batch_size, 1)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            train_correct += (preds == batch_labels).sum().item()\n",
        "            train_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss_sum / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # === Validation ===\n",
        "        net.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_labels in val_loader:\n",
        "                batch_inputs = batch_inputs.to(device)\n",
        "                batch_labels = batch_labels.to(device).float()\n",
        "\n",
        "                logits = net(batch_inputs)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "\n",
        "                val_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "                val_correct += (preds == batch_labels).sum().item()\n",
        "                val_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss_sum / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        if epoch % log_every == 0:\n",
        "            print(f\"Epoch {epoch:03d} | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history"
      ],
      "metadata": {
        "id": "HCA6aXtlnH6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c09945-3d00-457b-d161-0208f863cc4e"
      },
      "id": "HCA6aXtlnH6J",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Function"
      ],
      "metadata": {
        "id": "-P8_EMUj-tnp"
      },
      "id": "-P8_EMUj-tnp"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(train_loss, val_loss, train_acc, val_acc):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    axs[0].plot(train_loss, label=\"Train Loss\", color='blue')\n",
        "    axs[0].plot(val_loss, label=\"Val Loss\", color='orange')\n",
        "    axs[0].set_title(\"Loss per Epoch\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Average Loss\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    axs[1].plot(train_acc, label=\"Train Accuracy\", color='green')\n",
        "    axs[1].plot(val_acc, label=\"Val Accuracy\", color='red')\n",
        "    axs[1].set_title(\"Accuracy per Epoch\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mDuVP9J6zl0T"
      },
      "id": "mDuVP9J6zl0T",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Training"
      ],
      "metadata": {
        "id": "fTOMeRt_-2Th"
      },
      "id": "fTOMeRt_-2Th"
    },
    {
      "cell_type": "code",
      "source": [
        "net_16=Net_16()\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_16, train_loader, val_loader, epochs=200)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krgbt41P-5Aw",
        "outputId": "8cf7a5e9-fccc-4dd5-a2db-f232ab363582"
      },
      "id": "krgbt41P-5Aw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cpu\n",
            "Epoch 000 | Train Loss: 0.6816, Acc: 0.5661 | Val Loss: 0.6747, Acc: 0.5769\n",
            "Epoch 010 | Train Loss: 0.3866, Acc: 0.8560 | Val Loss: 0.3825, Acc: 0.8572\n",
            "Epoch 020 | Train Loss: 0.2740, Acc: 0.9026 | Val Loss: 0.2873, Acc: 0.8929\n",
            "Epoch 030 | Train Loss: 0.2271, Acc: 0.9185 | Val Loss: 0.2553, Acc: 0.9042\n",
            "Epoch 040 | Train Loss: 0.2006, Acc: 0.9280 | Val Loss: 0.2429, Acc: 0.9081\n",
            "Epoch 050 | Train Loss: 0.1823, Acc: 0.9349 | Val Loss: 0.2389, Acc: 0.9127\n",
            "Epoch 060 | Train Loss: 0.1684, Acc: 0.9402 | Val Loss: 0.2386, Acc: 0.9114\n",
            "Epoch 070 | Train Loss: 0.1569, Acc: 0.9447 | Val Loss: 0.2409, Acc: 0.9114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Aproach\n",
        "\n",
        "It doesn't improve beyond 91% accuracy. So maybe we can go simpler, then."
      ],
      "metadata": {
        "id": "_GUYSKp3HlRK"
      },
      "id": "_GUYSKp3HlRK"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 1\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #batch, words\n",
        "        features = self.embedding(index)             #batch, words, features\n",
        "        features = features.mean(-2)                 #batch, features\n",
        "        classifications = self.classifier(features)  #batch, 1\n",
        "        logits = classifications.squeeze(-1)         #batch\n",
        "        return logits\n",
        "\n",
        "net_1 = Net_1()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_1, train_loader, val_loader, epochs = 200)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "mNwmWLEuHrOD"
      },
      "id": "mNwmWLEuHrOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Polarity\n",
        "\n",
        "Since our embeddings have only 1 feature (`feature_cnt = 1`), each word is embedded to a scalar. We can interpret this scalar as a kind of sentiment polarity, especially since our model is trained for sentiment classification."
      ],
      "metadata": {
        "id": "mlg7DIdoXA_Y"
      },
      "id": "mlg7DIdoXA_Y"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the embedding weights as a NumPy array\n",
        "embedding_weights = net_1.embedding.weight.detach().cpu().numpy().squeeze()  # shape: (vocab_len,)\n",
        "\n",
        "# Reverse vocab dictionary to map indices back to words\n",
        "id2token = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# Skip <PAD> and <UNK> tokens (indices 0 and 1)\n",
        "valid_indices = np.array([idx for idx in range(2, len(embedding_weights)) if idx in id2token])\n",
        "valid_embeddings = embedding_weights[valid_indices]\n",
        "\n",
        "# Sort and select indices\n",
        "sorted_pos = np.argsort(-valid_embeddings)\n",
        "sorted_neg = np.argsort(valid_embeddings)\n",
        "sorted_neutral = np.argsort(np.abs(valid_embeddings))\n",
        "\n",
        "top_pos_indices = valid_indices[sorted_pos[:20]]\n",
        "top_neg_indices = valid_indices[sorted_neg[:20]]\n",
        "top_neutral_indices = valid_indices[sorted_neutral[:20]]\n",
        "\n",
        "# Print words and corresponding embedding values\n",
        "def print_words_with_embeddings(indices, title):\n",
        "    print(f\"\\n{title}\")\n",
        "    for idx in indices:\n",
        "        word = id2token[int(idx)]\n",
        "        value = embedding_weights[int(idx)]\n",
        "        print(f\"{word:15} -> {value:.4f}\")\n",
        "\n",
        "print_words_with_embeddings(top_pos_indices, \"Top 20 most positive words:\")\n",
        "print_words_with_embeddings(top_neg_indices, \"Top 20 most negative words:\")\n",
        "print_words_with_embeddings(top_neutral_indices, \"Top 20 most neutral words:\")\n"
      ],
      "metadata": {
        "id": "zPijTrZEXvM4"
      },
      "id": "zPijTrZEXvM4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificator\n",
        "\n",
        "It is interesting to see how the polar sentiment gets translated into the two  class values. Let's see:"
      ],
      "metadata": {
        "id": "ayZtmbdcvPyP"
      },
      "id": "ayZtmbdcvPyP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract weights and bias from the classifier layer\n",
        "classifier_weight = net_1.classifier.weight.detach().cpu().numpy()\n",
        "classifier_bias = net_1.classifier.bias.detach().cpu().numpy()\n",
        "\n",
        "print(\"Classifier weights (shape: {}):\".format(classifier_weight.shape))\n",
        "print(classifier_weight)\n",
        "\n",
        "print(\"\\nClassifier bias (shape: {}):\".format(classifier_bias.shape))\n",
        "print(classifier_bias)\n",
        "\n",
        "print(\"Recall our coding: \")\n",
        "print(label_map)\n"
      ],
      "metadata": {
        "id": "0Ra3BaKGv7ib"
      },
      "id": "0Ra3BaKGv7ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand-picked test sentences\n",
        "texts = [\n",
        "    \"I love this!\",\n",
        "    \"This is terrible.\",\n",
        "    \"Thank you so much!\",\n",
        "    \"I hate this.\",\n",
        "    \"Wow, awesome!\",\n",
        "    \"This sucks.\"\n",
        "]\n",
        "\n",
        "# Convert texts to input_ids using your tokenizer\n",
        "input_ids = process_texts(texts, vocab)\n",
        "input_tensor = torch.tensor(input_ids).to(next(net_1.parameters()).device)\n",
        "\n",
        "# Predict with trained model\n",
        "net_1.eval()\n",
        "with torch.no_grad():\n",
        "    logits = net_1(input_tensor).squeeze()\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Print results\n",
        "for text, prob in zip(texts, probs):\n",
        "    print(f\"{text:30} -> predicted probability of POSITIVE: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "00X0GbtFELx7"
      },
      "id": "00X0GbtFELx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Towards Attention!\n",
        "\n"
      ],
      "metadata": {
        "id": "cnwL4ci4aRMK"
      },
      "id": "cnwL4ci4aRMK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Entry Point\n",
        "\n",
        "The entry point is our Net_16 model with averaged features:"
      ],
      "metadata": {
        "id": "ZeX-Tnlra-pY"
      },
      "id": "ZeX-Tnlra-pY"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_16(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #SIZE: batch, words\n",
        "        features = self.embedding(index)             #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where AVERAGING takes place\n",
        "        features = features.mean(-2)                 #SIZE: batch, features\n",
        "\n",
        "\n",
        "        classifications = self.classifier(features)  #SIZE: batch, 1\n",
        "        logits = classifications.squeeze(-1)         #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "diAmuIhIbHOQ"
      },
      "id": "diAmuIhIbHOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Less Explicit Averaging"
      ],
      "metadata": {
        "id": "H-g5Q7KxbWzG"
      },
      "id": "H-g5Q7KxbWzG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is equivalent to taking the `mean()` over words:"
      ],
      "metadata": {
        "id": "KLDhT9JCd9gh"
      },
      "id": "KLDhT9JCd9gh"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        weights = torch.ones((batch, 1, words)) / words   # create EQUAL WEIGHT tensor summing to 1.0 ( words x (1/words) )\n",
        "                                                         #SIZE: batch, 1, words\n",
        "        features = weights @ features                    #SIZE: batch, 1, features\n",
        "\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "epoXI3SZbcHg"
      },
      "id": "epoXI3SZbcHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let us refactor this, taking the weighted average part into a separate `Attention` module:"
      ],
      "metadata": {
        "id": "OFcB29hteVru"
      },
      "id": "OFcB29hteVru"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, features):                         #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        weights = torch.ones((batch, 1, words)) / words   # create EQUAL WEIGHT tensor summing to 1.0 ( words x (1/words) )\n",
        "                                                         #SIZE: batch, 1, words\n",
        "        features = weights @ features                    #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "8FzeUuJ-ehns"
      },
      "id": "8FzeUuJ-ehns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that we did up until that point was to rewrite taking the `mean()` into a separate `Attention` which calculates the weighted averager with equal weights over weights."
      ],
      "metadata": {
        "id": "fKuofPqsfWxe"
      },
      "id": "fKuofPqsfWxe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Notion of Energy\n",
        "\n",
        "Another useful concept is that of the energy. The energy equal to 0 uniformly for all words translates (with the use of `softmax`) into the equal weights, so the below version is still equivalent to what we already had (but, arguably, it looks much more complex):"
      ],
      "metadata": {
        "id": "3cFkehdKf1vE"
      },
      "id": "3cFkehdKf1vE"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = torch.zeros((batch, 1, words))       #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "IVkMyyYjgN6p"
      },
      "id": "IVkMyyYjgN6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Something New - Learned Energies"
      ],
      "metadata": {
        "id": "8Q4efFtch23v"
      },
      "id": "8Q4efFtch23v"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.energy = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = self.energy(features)                #SIZE: batch, words, 1\n",
        "        energies = energies.transpose(-2, -1)           #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_16_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with LEARNED WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits\n",
        "\n",
        "net_att = Net_16_Towards_Attention()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att, train_loader, val_loader, epochs = 100)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "8VLpkoPbh9LG"
      },
      "id": "8VLpkoPbh9LG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}