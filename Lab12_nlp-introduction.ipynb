{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab12_nlp-introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 12 - Natural Language Processing - Introduction\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---------------\n"
      ],
      "metadata": {
        "id": "kzosqJ1czsY9"
      },
      "id": "kzosqJ1czsY9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Crowdsource Sentiment Dataset  \n",
        "--------------\n",
        "\n",
        "It features **43k sentences** with **human-annotated emotion labels** from real-world user-generated content such as feedback, reviews, and comments.\n",
        "\n",
        "Each sentence can be labeled with **one or more of 27 fine-grained emotion categories**, such as:\n",
        "- *joy*, *amusement*, *approval*, *sadness*, *anger*, *fear*, *realization*, *pride*, etc.,  \n",
        "plus an optional **neutral** label.\n",
        "\n",
        "Because the dataset is **multi-label**, a single sentence may express a combination of emotions (e.g., *pride* and *fear*).\n",
        "\n",
        "The dataset was released by Google as part of its **Crowdsource project** and is related to the **GoEmotions** initiative. It's particularly valuable for building models that understand nuanced emotional language in realistic user comments.\n",
        "\n"
      ],
      "metadata": {
        "id": "G7rPtPD1zvCV"
      },
      "id": "G7rPtPD1zvCV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Mapping emotions to sentiment\n",
        "\n",
        "To use the dataset for **sentiment classification**, a rule-based mapping is applied to reduce multi-label emotion annotations into a single sentiment label:\n",
        "\n",
        "- If **any emotion** in the label set is **positive** → classify as **Positive**\n",
        "- Else if **any emotion** in the label set is **negative** → classify as **Negative**\n",
        "- Else → classify as **Neutral**\n",
        "\n",
        "This decision rule captures the dominant emotional tone of each sentence, simplifying the dataset for use in binary or ternary sentiment classification tasks."
      ],
      "metadata": {
        "id": "eLVlq3YcVQ7u"
      },
      "id": "eLVlq3YcVQ7u"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "yVUhR__7NByz",
        "outputId": "6c69e120-e343-435b-f0e5-42f40ef22489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yVUhR__7NByz",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the GoEmotions dataset (by Google)\n",
        "dataset = load_dataset(\"go_emotions\")\n",
        "\n",
        "# Convert to binary sentiment (positive/neutral/negative)\n",
        "def map_to_sentiment(example):\n",
        "    positive = {\n",
        "        1,   # admiration\n",
        "        2,   # amusement\n",
        "        3,   # approval\n",
        "        7,   # caring\n",
        "        10,  # desire\n",
        "        12,  # excitement\n",
        "        14,  # gratitude\n",
        "        19,  # love\n",
        "        20,  # optimism\n",
        "        22,  # pride\n",
        "        23,  # relief\n",
        "        26   # joy\n",
        "    }\n",
        "\n",
        "    negative = {\n",
        "        4,   # anger\n",
        "        5,   # annoyance\n",
        "        6,   # disapproval\n",
        "        8,   # confusion\n",
        "        9,   # disappointment\n",
        "        11,  # embarrassment\n",
        "        13,  # fear\n",
        "        15,  # grief\n",
        "        16,  # nervousness\n",
        "        17,  # remorse\n",
        "        18  # sadness\n",
        "    }\n",
        "\n",
        "    neutral = {\n",
        "        0,   # neutral\n",
        "        21,  # curiosity (ambiguous, context-specific)\n",
        "        24,  # realization (realization label is often used for sentences that show understanding, acknowledgment, or reflection, without emotional intensity, like in “Oh, now I get what she meant” )\n",
        "        25,  # surprise (can be negative or positive, but often context-specific)\n",
        "        27   # none (used when no emotion is detected)\n",
        "    }\n",
        "    labels = set(example[\"labels\"])\n",
        "    if labels & positive:\n",
        "        return \"positive\"\n",
        "    elif labels & negative:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Apply sentiment mapping\n",
        "dataset = dataset[\"train\"].map(lambda x: {\"sentiment\": map_to_sentiment(x)})\n",
        "\n",
        "# Convert to DataFrame for easier use\n",
        "df = dataset.to_pandas()[[\"text\", \"sentiment\"]]\n",
        "\n",
        "print(df.sample(5))  # Show sample entries\n",
        "print(\"Rows:\", len(df))\n"
      ],
      "metadata": {
        "id": "E8GmG8SAzsE6",
        "outputId": "3f73ce5c-c2ec-43da-b371-eb9a5bda123b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "E8GmG8SAzsE6",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text sentiment\n",
            "11929  Dayyuuummm!!! If you ever get another one, you...  negative\n",
            "27426  > neoprene kayaking gloves Thanks will check t...  negative\n",
            "35543  Not cheating and not using exploits is honorab...  negative\n",
            "17869  Someone that can check your dash and aa your j...   neutral\n",
            "31134  watching [NAME] attempt hook shots gives me a ...  positive\n",
            "Rows: 43410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into 80% train and 20% validation\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "# Check the sizes\n",
        "print(f\"Training size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
        "print(train_df['sentiment'].value_counts(normalize=True))\n",
        "print(val_df['sentiment'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "BJeOvCCgSLWC",
        "outputId": "a6b715d3-0bed-4761-cf86-6fcfc0d20aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BJeOvCCgSLWC",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size: 34728, Validation size: 8682\n",
            "sentiment\n",
            "neutral     0.391384\n",
            "positive    0.329158\n",
            "negative    0.279457\n",
            "Name: proportion, dtype: float64\n",
            "sentiment\n",
            "neutral     0.391384\n",
            "positive    0.329187\n",
            "negative    0.279429\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "-------------------\n",
        "\n",
        "To feed text into a neural network, we need to represent words in a \"neural-network-ish\" way — that is, as numbers. The standard approach is to use a tokenizer, often from a pretrained model. However, since we plan to experiment with our own attention modules later on, **we’ll avoid using any pretrained tokenizer**.\n",
        "\n",
        "Instead, we’ll go with a simple, word-based tokenization. As part of this, we’ll clean the text by removing any non-standard HTML tags, digits, extra whitespace, and punctuation. We’ll also convert all words to lowercase to ensure consistency."
      ],
      "metadata": {
        "id": "T07SvDTYV6nO"
      },
      "id": "T07SvDTYV6nO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Tokens: `<PAD>` and `<UNK>`\n",
        "\n",
        "In our text preprocessing pipeline, we convert each word to a number using a vocabulary. Two special tokens help us handle padding and unknown words.\n"
      ],
      "metadata": {
        "id": "IyT2bcscZu4P"
      },
      "id": "IyT2bcscZu4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### `<PAD>` — Padding Token\n",
        "\n",
        "- Represents empty slots when we need all input sequences to be the same length.\n",
        "- Assigned index `0`.\n",
        "- Used so that batches of sentences can be processed together by the model.\n",
        "\n",
        "For example:  \n",
        "Original: `[17, 5, 23]`  \n",
        "Padded:   `[17, 5, 23, 0, 0]` (for a fixed length of 5)"
      ],
      "metadata": {
        "id": "Ha9i37gvaBUn"
      },
      "id": "Ha9i37gvaBUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### `<UNK>` — Unknown Token\n",
        "\n",
        "- Represents any word that is **not in the vocabulary**.\n",
        "- Assigned index `1`.\n",
        "- Occurs when:\n",
        "  1. A word was **too rare in the training data** (appeared only once and was excluded from the vocabulary).\n",
        "  2. A word appears **only in validation or test data**.\n",
        "\n",
        "> In our setup, we **excluded all words that appear only once** in the training set.  \n",
        "> So even in the training data, some tokens are replaced with `<UNK>`.  \n",
        "> These are called **rare unknowns** — they help the model learn how to handle unusual or unfamiliar words.\n",
        "\n",
        "\n",
        "By including `<UNK>` during training, we teach the model how to deal with unseen or rare words at test time — which is **crucial for generalization**.\n"
      ],
      "metadata": {
        "id": "3Rymjwd6ZsoK"
      },
      "id": "3Rymjwd6ZsoK"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "PAD_LEN = 32\n",
        "\n",
        "# Tokenize with cleaning\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)      # remove digits and punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()   # normalize whitespace\n",
        "    return text.split()\n",
        "\n",
        "# Tokenize text\n",
        "train_tokens = train_df['text'].apply(tokenize)\n",
        "val_tokens = val_df['text'].apply(tokenize)\n",
        "\n",
        "# Build vocabulary from training set — exclude rare words (freq = 1)\n",
        "token_counter = Counter(token for sentence in train_tokens for token in sentence)\n",
        "vocab = {\n",
        "    token: idx + 2\n",
        "    for idx, (token, count) in enumerate(token_counter.items())\n",
        "    if count > 1\n",
        "}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# Convert tokens to indices\n",
        "def tokens_to_indices(tokens, vocab):\n",
        "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "# Pad or truncate sequences to a fixed length\n",
        "def pad_sequence(seq, max_len=32, pad_value=0):\n",
        "    if len(seq) < max_len:\n",
        "        return seq + [pad_value] * (max_len - len(seq))\n",
        "    else:\n",
        "        return seq[:max_len]\n",
        "\n",
        "# Apply both steps to train and val sets\n",
        "\n",
        "train_df['input_ids'] = train_tokens.apply(lambda tokens: pad_sequence(tokens_to_indices(tokens, vocab), max_len=PAD_LEN))\n",
        "val_df['input_ids'] = val_tokens.apply(lambda tokens: pad_sequence(tokens_to_indices(tokens, vocab), max_len=PAD_LEN))\n",
        "\n",
        "# Example check\n",
        "print(train_df[['text', 'input_ids']].head(3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bNJLBhifYFhO",
        "outputId": "659e7542-479f-45cf-f260-b29a86898487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bNJLBhifYFhO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  \\\n",
            "21866  The guy that waived a future hall of famer. Ha...   \n",
            "5542                                          Stop what?   \n",
            "38431                                          Oh wow!!!   \n",
            "\n",
            "                                               input_ids  \n",
            "21866  [2, 3, 4, 1, 6, 7, 8, 9, 1, 11, 12, 13, 14, 15...  \n",
            "5542   [17, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
            "38431  [19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}