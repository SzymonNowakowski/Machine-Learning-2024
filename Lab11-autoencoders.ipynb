{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab11-autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 11 - Autoencoders\n",
        "### Author: Szymon Nowakowski"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Introduction\n",
        "--------------\n",
        "\n",
        "Autoencoders can be thought of as nonlinear extensions of PCA. In this class, we’ll train an autoencoder on the MNIST dataset and compare its encoded representation to the PCA space we constructed earlier (remember our very first class?). This comparison will help us see whether the autoencoder captures the structure of the data more effectively.\n",
        "\n",
        "Next, we’ll put the trained autoencoder to practical use. It is great in anomaly detection (identification of outliers) and image denoising.\n",
        "\n",
        "You’ll also notice that throughout this class, we’re treating the images in a class-diagnostic, unsupervised manner—focusing on the structure of the data itself, rather than on labels."
      ],
      "metadata": {
        "id": "fMvw34S08ZvN"
      },
      "id": "fMvw34S08ZvN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deterministic output\n",
        "--------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "VnC1WjOL5-Pd"
      },
      "id": "VnC1WjOL5-Pd"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n"
      ],
      "metadata": {
        "id": "0wwOGe0j5nMk"
      },
      "id": "0wwOGe0j5nMk",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading MNIST Dataset\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "jBj9rZpumQ2a"
      },
      "id": "jBj9rZpumQ2a"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot\n",
        "\n",
        "transform = torchvision.transforms.ToTensor() #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "\n",
        "     # note - we are NOT normalizing pixels, as we want to keep 0-1 range\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=8192,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=8192,\n",
        "                                         shuffle=False)\n",
        "\n",
        "testset_1 = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader_1 = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)\n"
      ],
      "metadata": {
        "id": "K_b3NgK0mT9C"
      },
      "id": "K_b3NgK0mT9C",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Sizes\n",
        "-------------------\n",
        "\n",
        "Recall:\n",
        "- Batched labels are of order one. The first (and only) index is a sample index within a batch. **The labels, however, are of no direct interest to us during this class**.\n",
        "- Image batches have order 4. The first index is a sample index within a batch, but a second index has size 1.\n",
        "  - This index represents a Channel number inserted here by `ToTensor()` transformation, always 0.\n",
        "  \n"
      ],
      "metadata": {
        "id": "KpN_zrBRmd6D"
      },
      "id": "KpN_zrBRmd6D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder and Decoder Networks\n",
        "-----------------\n",
        "\n",
        "Autoencoder is an Encoder followed by a Decoder, as in this diagram: ![Autoencoder](https://github.com/SzymonNowakowski/Machine-Learning-2024/raw/master/autoencoder_diagram.png\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "YAihBeAxndOm"
      },
      "id": "YAihBeAxndOm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "XOkTnHzLojq4"
      },
      "id": "XOkTnHzLojq4"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EncoderMLP(nn.Module):\n",
        "    def __init__(self, bottleneck_dimensionality):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()                        # 1x28x28 -> 784\n",
        "        self.linear1 = nn.Linear(784, 256)\n",
        "        self.linear2 = nn.Linear(256, 64)\n",
        "        self.linear3 = nn.Linear(64, bottleneck_dimensionality)  # -> bottleneck\n",
        "\n",
        "    def forward(self, x):              # B, 1, 28, 28             moreover, input data is in [0, 1]\n",
        "        x = self.flatten(x)            # B, 784\n",
        "        x = self.relu(self.linear1(x)) # B, 256\n",
        "        x = self.relu(self.linear2(x)) # B, 64\n",
        "        x = self.linear3(x)            # B, bottleneck\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JwvCLWJvp065"
      },
      "id": "JwvCLWJvp065",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "I4jGHOdaonuz"
      },
      "id": "I4jGHOdaonuz"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderMLP(nn.Module):\n",
        "    def __init__(self, bottleneck_dimensionality):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(bottleneck_dimensionality, 64)\n",
        "        self.linear2 = nn.Linear(64, 256)\n",
        "        self.linear3 = nn.Linear(256, 784)\n",
        "        self.sigmoid = nn.Sigmoid()             # Output in [0, 1] to mimick the true input data\n",
        "\n",
        "    def forward(self, x):                  # B, bottleneck\n",
        "        x = self.relu(self.linear1(x))     # B, 64\n",
        "        x = self.relu(self.linear2(x))     # B, 256\n",
        "        x = self.sigmoid(self.linear3(x))  # B, 784\n",
        "        return x.view(-1, 1, 28, 28)       # B, 1, 28, 28    - reshape to image size. -1 means \"infer the proper dimensionality automatically\"\n"
      ],
      "metadata": {
        "id": "3TpKt8o0o1YZ"
      },
      "id": "3TpKt8o0o1YZ",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder\n"
      ],
      "metadata": {
        "id": "KRGa98OBoqCF"
      },
      "id": "KRGa98OBoqCF"
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, bottleneck_dimensionality):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderMLP(bottleneck_dimensionality)\n",
        "        self.decoder = DecoderMLP(bottleneck_dimensionality)\n",
        "\n",
        "    def forward(self, x):      # B, 1, 28, 28\n",
        "        x = self.encoder(x)    # B, bottleneck\n",
        "        x = self.decoder(x)    # B, 1, 28, 28\n",
        "        return x"
      ],
      "metadata": {
        "id": "2c62Xk_xot2i"
      },
      "id": "2c62Xk_xot2i",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "----------------------\n",
        "\n",
        "Beware: Training takes around 10 minutes on a GPU, and closer to an hour on a CPU. I had the joy of discovering that firsthand when Colab temporarily revoked my GPU access."
      ],
      "metadata": {
        "id": "NFBpFy6FqLhL"
      },
      "id": "NFBpFy6FqLhL"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = Autoencoder(bottleneck_dimensionality=2).to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "EPOCHS = 128\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # === Train ===\n",
        "    net.train()\n",
        "    epoch_loss_sum = 0.0\n",
        "    epoch_sample_count = 0\n",
        "\n",
        "    for batch_inputs, _ in trainloader:\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = net(batch_inputs)\n",
        "\n",
        "        loss = F.mse_loss(reconstructed, batch_inputs, reduction='mean')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = batch_inputs.size(0)\n",
        "        epoch_loss_sum += loss.item() * batch_size\n",
        "        epoch_sample_count += batch_size\n",
        "\n",
        "    avg_train_loss = epoch_loss_sum / epoch_sample_count\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "\n",
        "    # === Evaluate on test set ===\n",
        "    net.eval()\n",
        "    test_loss_sum = 0.0\n",
        "    test_sample_count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, _ in testloader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            reconstructed = net(batch_inputs)\n",
        "            loss = F.mse_loss(reconstructed, batch_inputs, reduction='mean')\n",
        "\n",
        "            batch_size = batch_inputs.size(0)\n",
        "            test_loss_sum += loss.item() * batch_size\n",
        "            test_sample_count += batch_size\n",
        "\n",
        "    avg_test_loss = test_loss_sum / test_sample_count\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "\n",
        "    if epoch % 16 == 0:\n",
        "        print(f\"Epoch {epoch:03d} | Train Loss (averaged over the epoch): {avg_train_loss:.6f} | Test Loss (after the epoch): {avg_test_loss:.6f}\")\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "uyEA3Qk3qMj6",
        "outputId": "64002545-0340-4e4d-d328-94af857c72ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uyEA3Qk3qMj6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cuda\n",
            "Epoch 000 | Train Loss (averaged over the epoch): 0.216841 | Test Loss (after the epoch): 0.176748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_history, label=\"Train loss\", color='blue')\n",
        "plt.plot(test_loss_history, label=\"Test loss\", color='orange')\n",
        "plt.title(\"Autoencoder Loss per Epoch (Avg per Sample)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FPrz9Lxn0aLH"
      },
      "id": "FPrz9Lxn0aLH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Testing\n",
        "----------------------"
      ],
      "metadata": {
        "id": "E2IB8sviqsir"
      },
      "id": "E2IB8sviqsir"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of examples to show\n",
        "N = 20\n",
        "\n",
        "net.eval()\n",
        "fig, axs = plt.subplots(2, N, figsize=(2 * N, 4))\n",
        "\n",
        "test_iter = iter(testloader_1)\n",
        "for i in range(N):\n",
        "    img, _ = next(test_iter)\n",
        "    img = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        recon = net(img)\n",
        "\n",
        "    axs[0, i].imshow(img[0, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    axs[0, i].axis(\"off\")\n",
        "    axs[1, i].imshow(recon[0, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    axs[1, i].axis(\"off\")\n",
        "\n",
        "axs[0, 0].set_title(\"Original\")\n",
        "axs[1, 0].set_title(\"Reconstructed\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubj7gcEjrs50"
      },
      "id": "ubj7gcEjrs50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Space\n",
        "----------------------\n",
        "\n",
        "It is instructive to plot the bottleneck (for the test set) in a 2D plot. As a comparison, we will plot the PCA of the test set to the side."
      ],
      "metadata": {
        "id": "PmJMqeN0QoFB"
      },
      "id": "PmJMqeN0QoFB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# === Collect latent codes and raw data ===\n",
        "net.eval()\n",
        "all_latents = []\n",
        "all_labels = []\n",
        "raw_images = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs = inputs.to(device)\n",
        "        latents = net.encoder(inputs)\n",
        "\n",
        "        all_latents.append(latents.cpu())\n",
        "        all_labels.append(labels)\n",
        "        raw_images.append(inputs.cpu().view(inputs.size(0), -1))  # Flatten: (B, 784)\n",
        "\n",
        "all_latents = torch.cat(all_latents).numpy()\n",
        "all_labels = torch.cat(all_labels).numpy()\n",
        "raw_images = torch.cat(raw_images).numpy()\n",
        "\n",
        "# === PCA ===\n",
        "scaler = StandardScaler()\n",
        "raw_images_std = scaler.fit_transform(raw_images)\n",
        "pca = PCA(n_components=2)\n",
        "raw_images_2d = pca.fit_transform(raw_images_std)\n",
        "\n",
        "# === Plot side-by-side with proper colorbar ===\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Bottleneck\n",
        "sc1 = axes[0].scatter(all_latents[:, 0], all_latents[:, 1], c=all_labels, cmap=\"tab10\", s=10, alpha=0.7)\n",
        "axes[0].set_title(\"2D Bottleneck (Autoencoder)\")\n",
        "axes[0].set_xlabel(\"Latent dim 1\")\n",
        "axes[0].set_ylabel(\"Latent dim 2\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# PCA\n",
        "sc2 = axes[1].scatter(raw_images_2d[:, 0], raw_images_2d[:, 1], c=all_labels, cmap=\"tab10\", s=10, alpha=0.7)\n",
        "axes[1].set_title(\"PCA of Original Images (Flattened)\")\n",
        "axes[1].set_xlabel(\"PC1\")\n",
        "axes[1].set_ylabel(\"PC2\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Shared colorbar – place to the right of both subplots\n",
        "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
        "fig.colorbar(sc2, cax=cbar_ax, ticks=range(10), label='Digit Label')\n",
        "\n",
        "plt.subplots_adjust(right=0.9)  # make room for colorbar\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qN3DCG45QrW0"
      },
      "id": "qN3DCG45QrW0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16 Dimensional Bottleneck Network\n",
        "\n",
        "To experience a little bit more expressive network, let us now train another network, with a 16D bottleneck."
      ],
      "metadata": {
        "id": "JOpt2Y5qW75z"
      },
      "id": "JOpt2Y5qW75z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "\n",
        "How could we attempt plotting the 16D bottleneck of this network?"
      ],
      "metadata": {
        "id": "ixx3uyXUk184"
      },
      "id": "ixx3uyXUk184"
    },
    {
      "cell_type": "code",
      "source": [
        "#### TRAINING THE 16D BOTTLENECK\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "BOTTLENECK = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net_16 = Autoencoder(bottleneck_dimensionality=BOTTLENECK).to(device)\n",
        "optimizer = torch.optim.Adam(net_16.parameters(), lr=0.001)\n",
        "\n",
        "EPOCHS = 128\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # === Train ===\n",
        "    net_16.train()\n",
        "    epoch_loss_sum = 0.0\n",
        "    epoch_sample_count = 0\n",
        "\n",
        "    for batch_inputs, _ in trainloader:\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = net_16(batch_inputs)\n",
        "\n",
        "        loss = F.mse_loss(reconstructed, batch_inputs, reduction='mean')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = batch_inputs.size(0)\n",
        "        epoch_loss_sum += loss.item() * batch_size\n",
        "        epoch_sample_count += batch_size\n",
        "\n",
        "    avg_train_loss = epoch_loss_sum / epoch_sample_count\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "\n",
        "    # === Evaluate on test set ===\n",
        "    net_16.eval()\n",
        "    test_loss_sum = 0.0\n",
        "    test_sample_count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, _ in testloader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            reconstructed = net_16(batch_inputs)\n",
        "            loss = F.mse_loss(reconstructed, batch_inputs, reduction='mean')\n",
        "\n",
        "            batch_size = batch_inputs.size(0)\n",
        "            test_loss_sum += loss.item() * batch_size\n",
        "            test_sample_count += batch_size\n",
        "\n",
        "    avg_test_loss = test_loss_sum / test_sample_count\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "\n",
        "    if epoch % 16 == 0:\n",
        "        print(f\"Epoch {epoch:03d} | Train Loss (averaged over the epoch): {avg_train_loss:.6f} | Test Loss (after the epoch): {avg_test_loss:.6f}\")\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2GP3lTJuXhLP"
      },
      "id": "2GP3lTJuXhLP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### VISUALISING LEARNING PROGRESS\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_history, label=\"Train loss\", color='blue')\n",
        "plt.plot(test_loss_history, label=\"Test loss\", color='orange')\n",
        "plt.title(\"Autoencoder Loss per Epoch (Avg per Sample)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6PNGg8d9o-M_"
      },
      "id": "6PNGg8d9o-M_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SOME EXAMPLES\n",
        "\n",
        "N = 20 # Number of examples to show\n",
        "\n",
        "net_16.eval()\n",
        "fig, axs = plt.subplots(2, N, figsize=(2 * N, 4))\n",
        "\n",
        "test_iter = iter(testloader_1)\n",
        "for i in range(N):\n",
        "    img, _ = next(test_iter)\n",
        "    img = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        recon = net_16(img)\n",
        "\n",
        "    axs[0, i].imshow(img[0, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    axs[0, i].axis(\"off\")\n",
        "    axs[1, i].imshow(recon[0, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    axs[1, i].axis(\"off\")\n",
        "\n",
        "axs[0, 0].set_title(\"Original\")\n",
        "axs[1, 0].set_title(\"Reconstructed\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B1P0rwDhpCkF"
      },
      "id": "B1P0rwDhpCkF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### AND FINALY THE 16D BOTTLENECK PLOTTED (ONLY ITS 2 PRINCIPAL COMPONENTS GET PLOTTED)\n",
        "\n",
        "net.eval()\n",
        "net_16.eval()\n",
        "\n",
        "def get_latents(net, dataloader):\n",
        "    all_latents, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            latents = net.encoder(inputs)\n",
        "            all_latents.append(latents.cpu())\n",
        "            all_labels.append(labels)\n",
        "    return torch.cat(all_latents).numpy(), torch.cat(all_labels).numpy()\n",
        "\n",
        "# Get latent spaces\n",
        "latents_2d, labels = get_latents(net, testloader)\n",
        "latents_16d, _ = get_latents(net_16, testloader)\n",
        "\n",
        "# PCA: 16D to 2D\n",
        "latents_16d_pca = PCA(n_components=2).fit_transform(StandardScaler().fit_transform(latents_16d))\n",
        "\n",
        "# === Plot side-by-side with proper colorbar ===\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Left: true 2D bottleneck\n",
        "sc1 = axes[0].scatter(latents_2d[:, 0], latents_2d[:, 1], c=labels, cmap=\"tab10\", s=10, alpha=0.7)\n",
        "axes[0].set_title(\"2D Bottleneck (net)\")\n",
        "axes[0].set_xlabel(\"Latent dim 1\")\n",
        "axes[0].set_ylabel(\"Latent dim 2\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Right: PCA of 16D bottleneck\n",
        "sc2 = axes[1].scatter(latents_16d_pca[:, 0], latents_16d_pca[:, 1], c=labels, cmap=\"tab10\", s=10, alpha=0.7)\n",
        "axes[1].set_title(\"PCA of 16D Bottleneck (net_16)\")\n",
        "axes[1].set_xlabel(\"PC 1\")\n",
        "axes[1].set_ylabel(\"PC 2\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Shared colorbar – placed explicitly to the right\n",
        "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
        "fig.colorbar(sc2, cax=cbar_ax, ticks=range(10), label='Digit Label')\n",
        "\n",
        "plt.subplots_adjust(right=0.9)  # make room for colorbar\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W_-pWuPKpHfq"
      },
      "id": "W_-pWuPKpHfq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising\n",
        "----------------------\n",
        "\n",
        "**Denoising** refers to the process of removing noise from corrupted images.\n",
        "\n",
        "In many real-world scenarios, capturing a clean image is difficult or even impossible due to suboptimal conditions during image acquisition or transmission, which leads to the introduction of noise.\n",
        "\n",
        "An example of such conditions is taking a photo at night, motion blur, poor-quality sensors, or image degradation during transmission over unstable communication channels.\n",
        "\n",
        "An autoencoder trained on clean images learns a compact internal representation (in the bottleneck) that captures the essential structure of valid data — in our case, digits. This compressed representation acts as a kind of \"filter\", discarding the irrelevant details and retaining what makes the digit recognizable.\n",
        "\n",
        "So when you pass a noisy image through the encoder, the network will try to map it to the closest valid bottleneck it knows — that is, one that likely came from a clean digit. Then, when you decode from that bottleneck, the decoder outputs an image that matches what it thinks that bottleneck represents — effectively removing the noise in the process.\n",
        "\n",
        "This works well as long as the noise doesn’t distort the image beyond what the encoder can still recognize as a digit."
      ],
      "metadata": {
        "id": "CoK3XOzJl82d"
      },
      "id": "CoK3XOzJl82d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure evaluation mode\n",
        "net_16.eval()\n",
        "\n",
        "# Configuration\n",
        "noise_levels = [0.1, 0.2, 0.4]\n",
        "N = 6  # Number of samples\n",
        "\n",
        "# Collect N samples from testloader (assuming batch_size=1)\n",
        "samples = []\n",
        "for i, (img, _) in enumerate(testloader_1):\n",
        "    samples.append(img)\n",
        "    if len(samples) >= N:\n",
        "        break\n",
        "images = torch.cat(samples, dim=0).to(device)  # shape: (N, 1, 28, 28)\n",
        "\n",
        "def add_noise(imgs, level):\n",
        "    noise = torch.randn_like(imgs) * level\n",
        "    return torch.clamp(imgs + noise, 0., 1.)\n",
        "\n",
        "# We have:\n",
        "#   row 0 -> original images\n",
        "#   for each noise level -> 2 rows:\n",
        "#       row (noisy)\n",
        "#       row (reconstructed)\n",
        "\n",
        "num_rows = 1 + 2 * len(noise_levels)\n",
        "fig, axes = plt.subplots(num_rows, N, figsize=(N * 2, num_rows * 2))\n",
        "\n",
        "# --- Row 0: Original images ---\n",
        "for j in range(N):\n",
        "    axes[0, j].imshow(images[j, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    axes[0, j].axis(\"off\")\n",
        "\n",
        "# Give a title in the center of row 0\n",
        "if N > 0:\n",
        "    axes[0, N // 2].set_title(\"Original\", fontsize=12)\n",
        "\n",
        "# --- Rows per noise level ---\n",
        "for i, level in enumerate(noise_levels):\n",
        "    # Create noisy + denoised\n",
        "    noisy = add_noise(images, level)\n",
        "    with torch.no_grad():\n",
        "        denoised = net_16(noisy)\n",
        "\n",
        "    # Identify which rows in the subplot grid\n",
        "    noisy_row = 1 + 2 * i\n",
        "    denoised_row = noisy_row + 1\n",
        "\n",
        "    # Plot all N samples in each row\n",
        "    for j in range(N):\n",
        "        # Noisy\n",
        "        axes[noisy_row, j].imshow(noisy[j, 0].cpu().numpy(), cmap=\"gray\")\n",
        "        axes[noisy_row, j].axis(\"off\")\n",
        "\n",
        "        # Denoised\n",
        "        axes[denoised_row, j].imshow(denoised[j, 0].cpu().numpy(), cmap=\"gray\")\n",
        "        axes[denoised_row, j].axis(\"off\")\n",
        "\n",
        "    # Titles for each row (centered)\n",
        "    axes[noisy_row, N // 2].set_title(f\"Noised (σ={level})\", fontsize=12)\n",
        "    axes[denoised_row, N // 2].set_title(\"Reconstructed\", fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B0zDsM1anaKZ"
      },
      "id": "B0zDsM1anaKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detecion\n",
        "---------------\n",
        "\n",
        "**Anomaly detection** refers to identifying data samples that deviate significantly from what is considered normal or expected.\n",
        "\n",
        "In image-based tasks, anomalies can be images that do not resemble the training data, such as digits drawn in an unusual style, incorrect symbols, or even entirely different types of images.\n",
        "\n",
        "An autoencoder trained solely on clean and valid digits learns to reconstruct inputs that resemble the training distribution. It captures the core structure of these digits in its bottleneck representation and learns to map inputs through this structure.\n",
        "\n",
        "When such a model is presented with an anomalous image — one that differs significantly from the digits it has seen — it struggles to reconstruct it accurately. The bottleneck fails to capture meaningful structure, and the decoder produces a poor approximation of the input.\n",
        "\n",
        "This discrepancy between the original input and the reconstructed output — usually measured by the reconstruction error — serves as a signal: if the error is high, the input is likely anomalous. In this way, the autoencoder becomes a powerful tool for detecting unexpected or out-of-distribution inputs."
      ],
      "metadata": {
        "id": "4VCKEndSn_7K"
      },
      "id": "4VCKEndSn_7K"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "net_16.eval()\n",
        "N = 5  # How many images (best/worst) to present per label\n",
        "\n",
        "for current_label in range(10):\n",
        "    reconstruction_errors = []\n",
        "    originals = []\n",
        "    reconstructions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels in testloader:\n",
        "            # Filter out samples belonging to the current label\n",
        "            mask = (batch_labels == current_label)\n",
        "            if mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            # Keep only the relevant images\n",
        "            relevant_images = batch_inputs[mask].to(device)\n",
        "            # Forward pass\n",
        "            recon = net_16(relevant_images)\n",
        "\n",
        "            # Compute MSE per sample\n",
        "            errors = F.mse_loss(recon, relevant_images, reduction='none')\n",
        "            errors = errors.view(errors.size(0), -1).mean(dim=1)  # average MSE per image\n",
        "\n",
        "            reconstruction_errors.extend(errors.cpu().numpy())\n",
        "            originals.extend(relevant_images.cpu())\n",
        "            reconstructions.extend(recon.cpu())\n",
        "\n",
        "    # If no images with this label exist in testloader, skip\n",
        "    if len(reconstruction_errors) == 0:\n",
        "        print(f\"No samples found for label {current_label}\")\n",
        "        continue\n",
        "\n",
        "    # Convert to tensors for easier indexing\n",
        "    reconstruction_errors = torch.tensor(reconstruction_errors)\n",
        "    originals = torch.stack(originals)\n",
        "    reconstructions = torch.stack(reconstructions)\n",
        "\n",
        "    # Find indices of best and worst reconstructions\n",
        "    best_indices = torch.topk(-reconstruction_errors, N).indices  # negative for smallest\n",
        "    worst_indices = torch.topk(reconstruction_errors, N).indices\n",
        "\n",
        "    def show_comparisons(indices, title):\n",
        "        num_show = len(indices)\n",
        "        fig, axs = plt.subplots(2, num_show, figsize=(2 * num_show, 4))\n",
        "        for i, idx in enumerate(indices):\n",
        "            axs[0, i].imshow(originals[idx][0], cmap='gray')\n",
        "            axs[0, i].axis('off')\n",
        "            axs[1, i].imshow(reconstructions[idx][0], cmap='gray')\n",
        "            axs[1, i].axis('off')\n",
        "        axs[0, 0].set_title(\"Original\")\n",
        "        axs[1, 0].set_title(\"Reconstructed\")\n",
        "        fig.suptitle(f\"{title}\\n(Digit={current_label})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Show best reconstructions\n",
        "    show_comparisons(best_indices, \"Best Recognized Samples\")\n",
        "\n",
        "    # Show worst reconstructions\n",
        "    show_comparisons(worst_indices, \"Worst Recognized Samples (Anomalous)\")\n"
      ],
      "metadata": {
        "id": "FMLzlpLMohqE"
      },
      "id": "FMLzlpLMohqE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Understanding Deconvolution in Autoencoders**\n",
        "---------------\n",
        "\n",
        "In class, we worked with autoencoders built from multilayer perceptrons (MLPs). However, encoders are often constructed using convolutional architectures to better capture spatial patterns. In this assignment, you'll explore how the decoder can use deconvolutional (transposed convolution) layers to reverse and mirror the operations performed by the convolutional encoder.\n",
        "\n",
        "While convolutional encoders are relatively well understood, **decoding (or upsampling) the compressed representation** using **deconvolutional layers** (also known as **transposed convolutions**) often raises questions.\n",
        "\n",
        "This assignment is particularly relevant because deconvolution is a core component of the U-Net architecture, a prominent neural network used extensively in image segmentation tasks.\n",
        "\n",
        "Your main objective is to deeply understand **how transposed convolution layers work**, and explain them in both words and visuals.\n",
        "\n",
        "\n",
        "## **The Objective**\n",
        "\n",
        "Understand and clearly explain how **transposed convolutions** work. Use 2D transposed convolutions and a small grid of 2D points as a working example.\n",
        "\n",
        "You may need to do some additional reading to complete this assignment.\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "### 1. **Theory Exploration**\n",
        "\n",
        "Using markdown cells in your Colab notebook, answer the following:\n",
        "\n",
        "- What is a **transposed convolution**?\n",
        "- How does it differ from a regular convolution?\n",
        "- How does it upsample feature maps?\n",
        "- What are **stride**, **padding**, and **kernel size**, and how do they influence the result in a transposed convolution?\n",
        "- To earn full two points, your explanation must be detailed enough for a reader to reproduce the upsampling process step by step.\n",
        "\n",
        "\n",
        "### 2. **Manual Diagram (by your hand, not a generated image)**\n",
        "\n",
        "Carefully plan and draw **by hand** a diagram or a set of diagrams that:\n",
        "\n",
        "- Explain the process of using **transposed convolution**.\n",
        "- Use an example of a **small input grid of 2D points** which gets expanded into a larger output grid.\n",
        "- Explain how stride, padding, and the kernel shape affect the result.\n",
        "- Show intermediate steps of the operation, not just input and output.\n",
        "\n",
        "**Scan or photograph your diagram(s)**, and upload it to your **GitHub repository** for this course.\n",
        "\n",
        "Then embed it in your Colab notebook using markdown (you can find examples on *how to do it* in previous notebooks related to this class, e.g. the one on linear regression or the one on the MLP network).\n",
        "\n",
        "\n",
        "### 3. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n"
      ],
      "metadata": {
        "id": "4Ar3Nf6gOX4Y"
      },
      "id": "4Ar3Nf6gOX4Y"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}