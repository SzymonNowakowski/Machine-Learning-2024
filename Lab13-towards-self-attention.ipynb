{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab13-towards-self-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 13 - Towards Self-Attention (*pl. Samoatencja*)\n",
        "\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---------------\n",
        "\n",
        "In this class, we build on our understanding of attention by introducing **self-attention**—a powerful mechanism that allows each word in a sentence to attend to **every other word**, including itself. While basic attention required an external query (e.g., from a classifier or decoder), self-attention uses the words within a sequence to compute their mutual influence, enabling the model to capture dependencies regardless of distance or position.\n",
        "\n",
        "This mechanism lies at the heart of modern architectures like the Transformer. By the end of this class, you’ll understand how self-attention works, how it differs from earlier forms of attention, and why it enables models to process sequences in parallel while preserving word-to-word relationships.\n",
        "\n",
        "In our next class, we’ll combine self-attention with other components—like feedforward layers and residual connections—to build the full Transformer architecture.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kzosqJ1czsY9"
      },
      "id": "kzosqJ1czsY9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgments\n",
        "\n",
        "*I would like to express my gratitude to my colleague **Przemysław Olbratowski** for this elegant way of introducing attention, which I find both intuitive and pedagogically effective.*"
      ],
      "metadata": {
        "id": "HTjXnKFXwKuj"
      },
      "id": "HTjXnKFXwKuj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yelp Reviews Polarity Dataset  \n",
        "--------------\n",
        "\n",
        "We shall continue to work with **Yelp Reviews Polarity Dataset**.\n",
        "\n",
        "This dataset contains **over 560k full-text reviews** from Yelp, labeled for **binary sentiment**:  \n",
        "- **positive** (5-star reviews)  \n",
        "- **negative** (1-star reviews)  \n",
        "\n",
        "We will not use the full dataset, because it cannot be handled by Colab RAM.\n",
        "\n",
        "Each example is a **real user-generated review**, typically 2–5 sentences long, capturing clear and direct sentiment in natural language.  \n",
        "There are no ambiguous or neutral labels, making this dataset ideal for training and evaluating **binary sentiment classifiers**.\n",
        "\n",
        "The dataset was curated and released as part of the **FastText** and **Text Classification Benchmarks** by researchers at Facebook AI. It is widely used for benchmarking sentiment models in both academia and industry.\n"
      ],
      "metadata": {
        "id": "G7rPtPD1zvCV"
      },
      "id": "G7rPtPD1zvCV"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVUhR__7NByz",
        "outputId": "5a56222b-c9df-4f8d-d4f3-a0680ec26f8f"
      },
      "id": "yVUhR__7NByz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import collections\n",
        "import re                             # Regular Expressions: for text searching and cleaning\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Load Yelp Polarity dataset from Hugging Face (one-time use)\n",
        "raw = load_dataset(\"yelp_polarity\")\n",
        "\n",
        "N_TRAIN = 100_000\n",
        "N_TEST = 20_000   # VAL + TEST sets\n",
        "\n",
        "train_raw = raw[\"train\"].shuffle(seed=42).select(range(N_TRAIN))\n",
        "test_raw  = raw[\"test\"].shuffle(seed=42).select(range(N_TEST))\n",
        "\n",
        "\n",
        "# Extract plain Python lists for text and labels\n",
        "def to_lists(dataset):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for example in dataset:\n",
        "        texts.append(example[\"text\"])\n",
        "        labels.append(example[\"label\"])\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = to_lists(train_raw)\n",
        "test_texts, test_labels   = to_lists(test_raw)\n",
        "\n",
        "# Split test set into val/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    test_texts, test_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Show class counts\n",
        "def print_distribution(name, labels):\n",
        "    c = collections.Counter(labels)\n",
        "    total = sum(c.values())\n",
        "    print(f\"\\n{name} distribution:\")\n",
        "    for label in sorted(c.keys()):\n",
        "        print(f\"  {label}: {c[label]} ({c[label]/total:.2%})\")\n",
        "\n",
        "print_distribution(\"Train\", train_labels)\n",
        "print_distribution(\"Validation\", val_labels)\n",
        "print_distribution(\"Test\", test_labels)\n",
        "\n",
        "label_map = {0: \"negative\", 1: \"positive\"}\n"
      ],
      "metadata": {
        "id": "E8GmG8SAzsE6"
      },
      "id": "E8GmG8SAzsE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Artificial (Concatenated) Sentences\n",
        "-----------------\n",
        "\n",
        "Working with the same dataset provides a practical benchmark: we can now assess whether using more sophisticated attention mechanisms (such as self-attention) leads to measurable improvements over simpler approaches like mean-pooling or fixed attention.\n",
        "\n",
        "**To recap:**\n",
        "\n",
        "In this augmentation strategy, each original sentence is placed at the end of a newly formed input, preceded by a randomly chosen distractor sentence. The original label is retained, ensuring that every new example remains correctly annotated. This approach doubles the dataset size (token-wise) while preserving both the number of samples and the overall label distribution.\n",
        "\n",
        "The task now becomes more focused: the model must learn to attend to the main sentence and ignore the irrelevant prefix.\n",
        "\n",
        "- **With attention**: the model can assign higher weights to the meaningful part of the input and suppress noise, including any trailing `<PAD>` tokens.\n",
        "- **Without attention**: a mean-pooling model processes all tokens equally, making it harder to isolate the signal from the noise introduced by the distractor."
      ],
      "metadata": {
        "id": "PIBRlAPIQ6-3"
      },
      "id": "PIBRlAPIQ6-3"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "MAX_LEN = 256  # changing max len to accomodate longer sequences, also, longer sequences are harder for the mean network\n",
        "\n",
        "def pair_every_sentence_with_random(texts, labels, seed=42):\n",
        "    \"\"\"\n",
        "    For each sentence, append a randomly selected second sentence.\n",
        "    The label of the first is preserved.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    n = len(texts)\n",
        "\n",
        "    augmented_texts = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for i in range(n):\n",
        "        first = texts[i]\n",
        "        label = labels[i]\n",
        "\n",
        "        second = texts[random.randint(0, n - 1)]\n",
        "\n",
        "        combined = second + \" \" + first # we prepend the sentence with a random one, now\n",
        "\n",
        "        augmented_texts.append(combined)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "    return texts + augmented_texts, labels + augmented_labels\n",
        "\n",
        "\n",
        "train_texts, train_labels = pair_every_sentence_with_random(train_texts, train_labels)\n",
        "val_texts, val_labels     = pair_every_sentence_with_random(val_texts,   val_labels)\n",
        "test_texts, test_labels   = pair_every_sentence_with_random(test_texts,  test_labels)\n"
      ],
      "metadata": {
        "id": "-OO_A8qMQ_lp"
      },
      "id": "-OO_A8qMQ_lp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "-------------------\n",
        "\n",
        "We shall also reuse the same tokenizer."
      ],
      "metadata": {
        "id": "T07SvDTYV6nO"
      },
      "id": "T07SvDTYV6nO"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.split()\n",
        "\n",
        "def build_vocab(token_lists, min_freq=2):\n",
        "    counter = collections.Counter(token for tokens in token_lists for token in tokens)\n",
        "    vocab = {\n",
        "        token: idx + 2  # reserve 0: <PAD>, 1: <UNK>\n",
        "        for idx, (token, count) in enumerate(counter.items())\n",
        "        if count >= min_freq\n",
        "    }\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "def tokens_to_ids(tokens, vocab):\n",
        "    return [vocab.get(tok, vocab['<UNK>']) for tok in tokens]   #for unknown tokens return vocab['<UNK>'] (which is == 1)\n",
        "\n",
        "def pad(seq, max_len=128, pad_value=0):\n",
        "    return seq + [pad_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "\n",
        "def process_texts(texts, vocab, max_len=128):\n",
        "    return [pad(tokens_to_ids(tokenize(text), vocab), max_len) for text in texts]\n",
        "\n",
        "\n",
        "# Tokenize training set and build vocab\n",
        "train_tokens = [tokenize(t) for t in train_texts]\n",
        "vocab = build_vocab(train_tokens)\n",
        "\n",
        "# Process splits into padded input_ids\n",
        "train_ids = process_texts(train_texts, vocab, MAX_LEN)\n",
        "val_ids   = process_texts(val_texts, vocab, MAX_LEN)\n",
        "test_ids  = process_texts(test_texts, vocab, MAX_LEN)\n",
        "\n",
        "# Print 5 real examples: raw text, tokenized, and input IDs\n",
        "shown = 0\n",
        "for i in range(len(train_texts)):\n",
        "    if 1 in train_ids[i][:5]:  # 1 is <UNK>\n",
        "        print(f\"Original:   {train_texts[i]}\")\n",
        "        print(f\"Tokenized:  {train_tokens[i]}\")\n",
        "        print(f\"Input IDs:  {train_ids[i]}\\n\")\n",
        "        shown += 1\n",
        "        if shown >= 5:\n",
        "            break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bNJLBhifYFhO"
      },
      "id": "bNJLBhifYFhO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loaders, Training Loop and Plotting\n",
        "--------------------\n",
        "\n",
        "We reuse those as well."
      ],
      "metadata": {
        "id": "XDLaOuwegNXu"
      },
      "id": "XDLaOuwegNXu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "7r9Y900tPaPT"
      },
      "id": "7r9Y900tPaPT"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Convert all to tensors\n",
        "def to_loader(input_ids, labels, batch_size=1024, shuffle=False):\n",
        "    x_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
        "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_tensor, y_tensor), batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = to_loader(train_ids, train_labels, BATCH_SIZE, shuffle=True)\n",
        "val_loader   = to_loader(val_ids, val_labels, BATCH_SIZE)\n",
        "test_loader  = to_loader(test_ids, test_labels, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "432ev7Jxg9Fw"
      },
      "id": "432ev7Jxg9Fw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop Function"
      ],
      "metadata": {
        "id": "7kZDRRGS-yMI"
      },
      "id": "7kZDRRGS-yMI"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "def train_model(net, train_loader, val_loader, epochs=1000, lr=0.001, log_every=10):\n",
        "    print(f\"Working on {device}\")\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_inputs, batch_labels in train_loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device).float()  # shape: (batch_size)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = net(batch_inputs)  # logits shape: (batch_size, 1)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            train_correct += (preds == batch_labels).sum().item()\n",
        "            train_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss_sum / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # === Validation ===\n",
        "        net.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_labels in val_loader:\n",
        "                batch_inputs = batch_inputs.to(device)\n",
        "                batch_labels = batch_labels.to(device).float()\n",
        "\n",
        "                logits = net(batch_inputs)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "\n",
        "                val_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "                val_correct += (preds == batch_labels).sum().item()\n",
        "                val_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss_sum / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        if epoch % log_every == 0:\n",
        "            print(f\"Epoch {epoch:03d} | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history"
      ],
      "metadata": {
        "id": "HCA6aXtlnH6J"
      },
      "id": "HCA6aXtlnH6J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Function"
      ],
      "metadata": {
        "id": "-P8_EMUj-tnp"
      },
      "id": "-P8_EMUj-tnp"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(train_loss, val_loss, train_acc, val_acc):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    axs[0].plot(train_loss, label=\"Train Loss\", color='blue')\n",
        "    axs[0].plot(val_loss, label=\"Val Loss\", color='orange')\n",
        "    axs[0].set_title(\"Loss per Epoch\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Average Loss\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    axs[1].plot(train_acc, label=\"Train Accuracy\", color='green')\n",
        "    axs[1].plot(val_acc, label=\"Val Accuracy\", color='red')\n",
        "    axs[1].set_title(\"Accuracy per Epoch\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mDuVP9J6zl0T"
      },
      "id": "mDuVP9J6zl0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n",
        "-----------------------\n",
        "\n",
        "We shall also reuse positional encoding."
      ],
      "metadata": {
        "id": "GnWwejPa68Fi"
      },
      "id": "GnWwejPa68Fi"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "pos_cnt = 16\n",
        "positional_encoding_B = 1000\n",
        "\n",
        "def sinusoid_positions(max_len = MAX_LEN, dim = pos_cnt):\n",
        "    pos = torch.arange(max_len, device=device).float().unsqueeze(1)\n",
        "    i   = torch.arange(dim, device=device).float().unsqueeze(0)\n",
        "    angle = pos / (positional_encoding_B ** (2 * (i//2) / dim))\n",
        "    S = torch.zeros(max_len, dim, device=device)\n",
        "    S[:, 0::2] = torch.sin(angle[:, 0::2])\n",
        "    S[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "    return S          #  SIZE: words, features  (constant matrix throuoght all computations)\n",
        "\n"
      ],
      "metadata": {
        "id": "oz5NJuqA7qoT"
      },
      "id": "oz5NJuqA7qoT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Create sinusoidal matrix S\n",
        "S = sinusoid_positions() # size: words, features\n",
        "\n",
        "# Separate sine (even) and cosine (odd) components\n",
        "sine = S[:, ::2]\n",
        "cosine = S[:, 1::2]\n",
        "gap = torch.full((S.size(0), 1), float('nan')).to(device)  # vertical separator\n",
        "\n",
        "# Concatenate: [sine | gap | cosine]\n",
        "S_sep = torch.cat([sine, gap, cosine], dim=1)\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(S_sep.cpu().numpy(), cmap=\"coolwarm\", cbar=True, xticklabels=2, yticklabels=4)\n",
        "plt.title(\"Sinusoidal Positional Encodings (Sine | Cosine)\")\n",
        "plt.xlabel(\"Embedding dimension (sine for even | gap | cosine for odd)\")\n",
        "plt.ylabel(\"Token position\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(S.cpu().numpy(), cmap=\"coolwarm\", cbar=True, xticklabels=2, yticklabels=4)\n",
        "plt.title(\"Sinusoidal Positional Encodings\")\n",
        "plt.xlabel(\"Embedding dimension\")\n",
        "plt.ylabel(\"Token position\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "R3PFWarM7O7C"
      },
      "id": "R3PFWarM7O7C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recall the Attention Network\n",
        "-----------------------"
      ],
      "metadata": {
        "id": "vJHdrJ8GUYxK"
      },
      "id": "vJHdrJ8GUYxK"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cnt = 16          # word embeddings\n",
        "pos_cnt = 16            # pos embeddings\n",
        "feature_cnt = 16\n",
        "\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.energy = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = self.energy(features)                #SIZE: batch, words, 1\n",
        "        energies = energies.transpose(-2, -1)           #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_Attention_And_PE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, embed_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index) + S             #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with LEARNED WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits\n",
        "\n",
        "net_att_pe = Net_Attention_And_PE()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att_pe, train_loader, val_loader, epochs = 20)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "UyIkrhVH8FPD"
      },
      "id": "UyIkrhVH8FPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Towards Self-Attention!\n",
        "--------------------\n",
        "\n",
        "We'll follow the same gradual approach as in the previous class.\n",
        "\n",
        "At first, some changes may seem minor—perhaps even redundant—sometimes resulting in models that are equivalent, or worse: equivalent *and* overparameterized. But each step brings us closer to our goal.\n",
        "\n",
        "And then, with one decisive twist, everything will fall into place.\n",
        "\n",
        "So off we go!\n"
      ],
      "metadata": {
        "id": "rtoGDOMtDeoX"
      },
      "id": "rtoGDOMtDeoX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Query and the Keys\n",
        "\n",
        "\n",
        "So far, our attention mechanism has relied on letting the **Query** (which we previously called ***Energy*** but it is the same thing\\) attend directly to the feature representations. But what if those raw features aren't always in the best form for comparison?\n",
        "\n",
        "Instead of having the Query interact with features directly, we can transform each feature into a new vector—one that’s specifically designed to interact with the Query. This brings us to the concept of **Keys**.\n",
        "\n",
        "Keys are learned projections of features that serve as their \"presentation\" when being evaluated by a Query. The idea is that attention will work better when both the **thing doing the looking** (Query) and the **thing being looked at** (Key) speak the same language.\n",
        "\n",
        "Formally, we define the Key as:\n",
        "\n",
        "$$\n",
        "\\text{key} = \\text{feature} \\cdot K\n",
        "$$\n",
        "\n",
        "where $K$ is a learnable matrix of shape $d \\times d$, and $d$ is the dimensionality of the input features.\n"
      ],
      "metadata": {
        "id": "FB7Y7KLQrxps"
      },
      "id": "FB7Y7KLQrxps"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cnt = 16          # word embeddings\n",
        "pos_cnt = 16            # pos embeddings\n",
        "feature_cnt = 16\n",
        "\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention_With_Keys(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.query = torch.nn.Linear(feature_cnt, 1)            # rename energy to query from now on\n",
        "        self.K = torch.nn.Linear(feature_cnt, feature_cnt)      # and introduce the K (key) matrix d x d\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        keys = self.K(features)                         #SIZE: batch, words, features\n",
        "        energies = self.query(keys)                     #SIZE: batch, words, 1\n",
        "        energies = energies.transpose(-2, -1)           #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_Attention_With_Keys(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, embed_cnt)\n",
        "        self.attention = Attention_With_Keys()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index) + S             #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with LEARNED WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "bDUyvwdUr1eG"
      },
      "id": "bDUyvwdUr1eG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task\n",
        "\n",
        "Obviously, this query-key formulation is not only equivalent to the regular energy-focused attention. It is redundant in terms of parameters. Do you see that?\n",
        "\n"
      ],
      "metadata": {
        "id": "2G5w4ilLuMUH"
      },
      "id": "2G5w4ilLuMUH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query-Key vs. Energy Formulation Comparison\n",
        "\n",
        "Let’s compare two formulations for computing attention energies from feature vectors.\n"
      ],
      "metadata": {
        "id": "KNNjPUuzwKJb"
      },
      "id": "KNNjPUuzwKJb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query-Key Factorized Version\n",
        "\n",
        "In our current model, we compute:\n",
        "\n",
        "$$\n",
        "\\text{keys} = \\text{features} \\cdot K \\\\\n",
        "\\text{energies} = \\text{keys} \\cdot q = (\\text{features} \\cdot K) \\cdot q = \\text{features} \\cdot (K \\cdot q)\n",
        "$$\n",
        "\n",
        "So although we introduce **Keys** and **Queries** explicitly, the final result is still a single linear projection of the original features:\n",
        "\n",
        "$$\n",
        "\\text{energies} = \\text{features} \\cdot W, \\quad \\text{where} \\quad W = K \\cdot q\n",
        "$$\n",
        "\n",
        "This is functionally equivalent to the classic **energy-only** attention mechanism."
      ],
      "metadata": {
        "id": "f8KJN-1EwRSr"
      },
      "id": "f8KJN-1EwRSr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Parameter Count\n",
        "\n",
        "Let $d$ be the dimensionality of the features (i.e., `feature_cnt`).\n",
        "\n",
        "- **Query-Key version:**\n",
        "  - $K$: a $d \\times d$ matrix, resulting in $d^2$ parameters\n",
        "  - $q$: a $d \\times 1$ vector, resulting in $d$ parameters  \n",
        "  \n",
        "  **Total: $d^2 + d$ parameters**\n",
        "\n",
        "- **Energy-only version:**\n",
        "  - A single linear layer from features to scalar\n",
        "  \n",
        "  **Total: $d$ parameters**\n",
        "\n",
        "So, in this setup, the query-key formulation has **$d^2$ more parameters** than necessary and is **strictly overparameterized**.\n"
      ],
      "metadata": {
        "id": "HrkAFKdIwaEa"
      },
      "id": "HrkAFKdIwaEa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "This setup offers **no additional expressivity**. In fact, unless carefully initialized or regularized, it can make learning harder and less stable due to the redundant parameterization.\n"
      ],
      "metadata": {
        "id": "hgLA6rgbueHP"
      },
      "id": "hgLA6rgbueHP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low-Rank Factorization\n",
        "\n",
        "Not all is lost. By carefully controlling the dimensionality of the **Key space**, we can reduce the number of parameters and limit overparameterization.\n",
        "\n",
        "Let’s introduce a **low-rank factorization**:\n",
        "\n",
        "- $K$: a $d \\times k$ matrix  \n",
        "- $q$: a $k \\times 1$ vector  \n",
        "- where $k < d$\n",
        "\n",
        "Then the energy computation becomes:\n",
        "\n",
        "$$\n",
        "\\text{keys} = \\text{features} \\cdot K  \\\\\n",
        "\\text{energies} = \\text{keys} \\cdot q = \\text{features} \\cdot (K \\cdot q)\n",
        "$$\n",
        "\n",
        "So again:\n",
        "\n",
        "$$\n",
        "\\text{energies} = \\text{features} \\cdot W, \\quad \\text{with } W = K \\cdot q \\in \\mathbb{R}^{d \\times 1}\n",
        "$$"
      ],
      "metadata": {
        "id": "vJQ8ud2Cx6B5"
      },
      "id": "vJQ8ud2Cx6B5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "####  Parameter Count\n",
        "\n",
        "Let’s compare the number of parameters:\n",
        "\n",
        "- **Low-rank query-key version:**\n",
        "  - $K$: $d \\times k$ → $dk$ parameters\n",
        "  - $q$: $k \\times 1$ → $k$ parameters  \n",
        "  **Total: $dk + k$ parameters**\n",
        "\n",
        "- **Original energy-only version:**\n",
        "  - One linear projection: $d$ parameters\n",
        "\n",
        "Notably, when $k = 1$, the number of parameters becomes:\n",
        "\n",
        "  $$\n",
        "  d \\cdot 1 + 1 = d + 1\n",
        "  $$\n",
        "\n",
        "  —which **exactly matches** the parameter count of the original energy-only formulation with a single linear projection.\n"
      ],
      "metadata": {
        "id": "aX9LAar8x-_V"
      },
      "id": "aX9LAar8x-_V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion\n",
        "\n",
        "We started off with a version that was **theoretically equivalent but practically worse** than the original energy-based attention. By factorizing the energy computation into separate **Key** and **Query** components without reducing dimensionality, we introduced unnecessary overparameterization—making the model **less stable** and harder to train.\n",
        "\n",
        "However, this detour was not without value: it led us to a more flexible and efficient formulation. By **reducing the dimensionality** of the key space (i.e., using a smaller $k$), we arrive at a **low-rank factorization** that:\n",
        "\n",
        "- Matches the original model’s parameter count when $k = 1$,\n",
        "- And enables richer attention structures when $k > 1$.\n",
        "\n",
        "So while our initial formulation was redundant, it ultimately revealed a **useful and scalable path forward**.\n",
        "\n",
        "We will not be training this version, as thanks to this careful analysis, we do not expect nothing valuable to come out of it.\n"
      ],
      "metadata": {
        "id": "zEiE6Wafy8vF"
      },
      "id": "zEiE6Wafy8vF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Queries\n",
        "\n",
        "Up to now, we’ve used a **single, global query** to compute attention scores over all tokens in the sequence. This gave us a fixed viewpoint—useful for tasks like classification—but it limited the model's ability to express interactions between tokens.\n",
        "\n",
        "In **self-attention**, we take this idea further: **each token generates its own query**.\n",
        "\n",
        "A **Query** is a learned projection of a token’s features that expresses **what this token is looking for**. Instead of one global viewpoint, every token forms its own question and compares it against the rest of the sequence.\n",
        "\n",
        "To do this, we introduce a learnable matrix $Q$ of shape $d \\times k$, where:\n",
        "- $d$ is the dimensionality of the feature vector,\n",
        "- $k$ is the dimensionality of the shared **query and key space**.\n",
        "\n",
        "Each query is computed as:\n",
        "\n",
        "$$\n",
        "\\text{query} = \\text{feature} \\cdot Q\n",
        "$$\n",
        "\n",
        "Likewise, each key is computed as:\n",
        "\n",
        "$$\n",
        "\\text{key} = \\text{feature} \\cdot K\n",
        "$$\n",
        "\n",
        "where $K$ is another learned matrix of shape $d \\times k$.\n",
        "\n",
        "We then compute attention scores using a dot product between the query of one token and the keys of all tokens in the sequence:\n",
        "\n",
        "$$\n",
        "\\text{energy}_{i,j} = \\text{query}_i \\cdot \\text{key}_j\n",
        "$$\n",
        "\n",
        "This produces a separate attention distribution **for each token**—each one deciding independently which other tokens to attend to.\n",
        "\n",
        "This mechanism is the heart of self-attention: it turns every token into an active agent that selectively gathers information from its surroundings.\n"
      ],
      "metadata": {
        "id": "YwOOhNG801TX"
      },
      "id": "YwOOhNG801TX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Formulation\n",
        "\n",
        "\n",
        "Let’s now express this more generally in matrix form.\n",
        "\n",
        "Let:\n",
        "- $F \\in \\mathbb{R}^{n \\times d}$ be the matrix of input features (for $n$ tokens, each of dimension $d$),\n",
        "- $Q \\in \\mathbb{R}^{d \\times k}$ be the **query projection matrix**,\n",
        "- $K \\in \\mathbb{R}^{d \\times k}$ be the **key projection matrix**.\n",
        "\n",
        "We compute:\n",
        "- Queries: $F Q \\in \\mathbb{R}^{n \\times k}$\n",
        "- Keys: $F K \\in \\mathbb{R}^{n \\times k}$\n",
        "\n",
        "The attention **energy matrix** is then:\n",
        "\n",
        "$$\n",
        "E = (F Q)(F K)^\\top \\in \\mathbb{R}^{n \\times n}\n",
        "$$\n",
        "\n",
        "Now comes the crucial observation:\n",
        "\n",
        "We can factor the expression as:\n",
        "\n",
        "$$\n",
        "E = F (Q K^\\top) F^\\top\n",
        "$$\n",
        "\n",
        "This shows that the full $n \\times n$ energy matrix is a **quadratic form** defined by the product $Q K^\\top \\in \\mathbb{R}^{d \\times d}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "wjS5Gyvr2YYw"
      },
      "id": "wjS5Gyvr2YYw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Count\n",
        "\n",
        "- $Q$: $d \\times k$, resulting in $dk$ parameters  \n",
        "- $K$: $d \\times k$, resulting in $dk$ parameters  \n",
        "\n",
        "**Total: $2dk$ parameters**\n",
        "\n",
        "- The matrix $Q K^\\top \\in \\mathbb{R}^{d \\times d}$ is a **learned, structured weight matrix**, but it is **not arbitrary** — it is parameterized by just $2dk$ parameters, not $d^2$.\n",
        "\n",
        "So although $Q K^\\top$ is a $d \\times d$ matrix, it has **rank at most $k$**, and it is **not directly learned**, but **composed** from two lower-rank projections."
      ],
      "metadata": {
        "id": "KeE4SjRb3Kt_"
      },
      "id": "KeE4SjRb3Kt_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "- The energy matrix $E \\in \\mathbb{R}^{n \\times n}$ is expressed as:\n",
        "  $$\n",
        "  E = F (Q K^\\top) F^\\top\n",
        "  $$\n",
        "- This shows that attention energies are governed by a **global bilinear interaction** over the features.\n",
        "- The structure $Q K^\\top$ has rank at most $k$ and is fully determined by $2dk$ parameters.\n",
        "- For $k \\ll d$, this formulation yields a **low-rank attention mechanism** that is both expressive and efficient — far more efficient than a full, unconstrained $d \\times d$ attention matrix."
      ],
      "metadata": {
        "id": "xHtLSYnu3Rpu"
      },
      "id": "xHtLSYnu3Rpu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task\n",
        "\n",
        "Take this code of `Attention_With_Keys` and add *Queries* to it as well"
      ],
      "metadata": {
        "id": "JcvQ5p1v4dzn"
      },
      "id": "JcvQ5p1v4dzn"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cnt = 16          # word embedding dimension\n",
        "pos_cnt = 16            # positional encoding dimension\n",
        "feature_cnt = 16        # total feature dimension (d)\n",
        "k = 8                   # query/key dimension (low-rank)\n",
        "\n",
        "vocab_len = max(vocab.values()) + 1  # maximal value (index) of a token\n",
        "\n",
        "class SelfAttention_QK(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Q = torch.nn.Linear(feature_cnt, k)        # query projection\n",
        "        self.K = torch.nn.Linear(feature_cnt, k)        # key projection\n",
        "\n",
        "    def forward(self, features):                        # size: batch, words, d\n",
        "        queries = self.Q(features)                      # size: batch, words, k\n",
        "        keys = self.K(features)                         # size: batch, words, k\n",
        "\n",
        "        # Compute energy scores:\n",
        "        energies = queries @ keys.transpose(-2, -1)     # size: batch, words, words\n",
        "\n",
        "        # Normalize energies into attention weights\n",
        "        attention = F.softmax(energies, dim=-1)         # size: batch, words, words\n",
        "\n",
        "        # No values yet — return attention matrix for now\n",
        "        return attention\n",
        "\n",
        "class Net_SelfAttention_QK(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, embed_cnt)\n",
        "        self.attention = SelfAttention_QK()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "\n",
        "    def forward(self, index):                            # size: batch, words\n",
        "        features = self.embedding(index) + S             # size: batch, words, d\n",
        "        attention = self.attention(features)             # size: batch, words, words\n",
        "\n",
        "        # Temporary: pool original features using attention weights\n",
        "        pooled = attention @ features                    # size: batch, words, d\n",
        "        pooled = pooled.mean(dim=1, keepdim=True)        # size: batch, 1, d\n",
        "\n",
        "        output = self.classifier(pooled)                 # size: batch, 1, 1\n",
        "        logits = output.squeeze(-1).squeeze(-1)          # size: batch\n",
        "        return logits\n",
        "\n",
        "# Instantiate and train\n",
        "net_att = Net_SelfAttention_QK()\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att, train_loader, val_loader, epochs=20)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)\n"
      ],
      "metadata": {
        "id": "BnIHt_Vo4oab"
      },
      "id": "BnIHt_Vo4oab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Values\n",
        "\n",
        "So far, we’ve seen how **Queries** and **Keys** determine *where* attention should be paid — that is, which tokens are relevant to each other. But once the model has computed these attention **energies**, what exactly is being retrieved?\n",
        "\n",
        "This is the role of **Values**.\n",
        "\n",
        "Each token is associated with a **value vector** — a projection of its features that represents the *content* it offers if attended to. These are computed just like queries and keys:\n",
        "\n",
        "$$\n",
        "\\text{value} = \\text{feature} \\cdot V\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{feature} \\in \\mathbb{R}^d$ is the input feature vector for a single token,\n",
        "- $V \\in \\mathbb{R}^{d \\times d_v}$ is a learnable weight matrix,\n",
        "- $\\text{value} \\in \\mathbb{R}^{d_v}$ is the resulting value vector.\n",
        "\n",
        "For a batch of $n$ tokens, this gives:\n",
        "\n",
        "$$\n",
        "\\text{values} = F \\cdot V \\in \\mathbb{R}^{n \\times d_v}\n",
        "$$\n",
        "\n",
        "The attention mechanism computes a matrix of **energies**:\n",
        "\n",
        "$$\n",
        "E = F \\cdot (QK^\\top) \\cdot F^\\top \\in \\mathbb{R}^{n \\times n}\n",
        "$$\n",
        "\n",
        "These energies encode which tokens should influence each other — but not what is being transferred. To actually compute the result of attention, we combine these (after row-wise normalization) with the value vectors:\n",
        "\n",
        "$$\n",
        "\\text{output} = \\text{weights} \\cdot \\text{values}\n",
        "$$\n",
        "\n",
        "where `weights` is obtained from $E$ by applying softmax or another normalization function row-wise.\n",
        "\n"
      ],
      "metadata": {
        "id": "laxc4meczign"
      },
      "id": "laxc4meczign"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Now we have transitioned to self-attention, where each token gets:\n",
        "\n",
        "- Its own query (what it's looking for),\n",
        "- Its own key (how it presents itself),\n",
        "- Its own value (what it offers if attended to)"
      ],
      "metadata": {
        "id": "on_Eyc3u0I21"
      },
      "id": "on_Eyc3u0I21"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cnt = 16          # word embedding dimension\n",
        "pos_cnt = 16            # positional encoding dimension\n",
        "feature_cnt = 16        # total feature dimension (d)\n",
        "k = 8                   # query/key dimension (low-rank)\n",
        "d_v = feature_cnt       # output value dimension (could be different if desired)\n",
        "\n",
        "vocab_len = max(vocab.values()) + 1 # maximal value (index) of a token\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Q = torch.nn.Linear(feature_cnt, k)        # query projection\n",
        "        self.K = torch.nn.Linear(feature_cnt, k)        # key projection\n",
        "        self.V = torch.nn.Linear(feature_cnt, d_v)      # value projection\n",
        "\n",
        "    def forward(self, features):                        # size: batch, words, d\n",
        "        queries = self.Q(features)                      # size: batch, words, k\n",
        "        keys = self.K(features)                         # size: batch, words, k\n",
        "        values = self.V(features)                       # size: batch, words, d_v\n",
        "\n",
        "        # Compute energy scores:\n",
        "        energies = queries @ keys.transpose(-2, -1)     # size: batch, words, words\n",
        "\n",
        "        # Normalize energies into attention weights\n",
        "        attention = F.softmax(energies, dim=-1)         # size: batch, words, words)\n",
        "\n",
        "        # Apply attention weights to values:\n",
        "        output = attention @ values                     # size: batch, words, d_v\n",
        "\n",
        "        return output\n",
        "\n",
        "class Net_SelfAttention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, embed_cnt)\n",
        "        self.attention = SelfAttention()\n",
        "        self.classifier = torch.nn.Linear(d_v, 1)\n",
        "\n",
        "    def forward(self, index):                            # size: batch, words\n",
        "        features = self.embedding(index) + S             # size: batch, words, d\n",
        "        attended = self.attention(features)              # size: batch, words, d_v\n",
        "        pooled = attended.mean(dim=1, keepdim=True)      # size: batch, 1, d_v  # average across words\n",
        "        output = self.classifier(pooled)                 # size: batch, 1, 1\n",
        "        logits = output.squeeze(-1).squeeze(-1)          # size: batch\n",
        "        return logits\n",
        "\n",
        "# Instantiate and train\n",
        "net_att = Net_SelfAttention()\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att, train_loader, val_loader, epochs=20)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)\n"
      ],
      "metadata": {
        "id": "84yGNQG56e2j"
      },
      "id": "84yGNQG56e2j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}