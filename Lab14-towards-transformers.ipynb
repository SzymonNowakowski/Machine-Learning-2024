{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab14-towards-transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 14 - Towards Transformer\n",
        "\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---------------\n",
        "In this class, we take our first steps into Natural Language Processing (NLP). We'll begin by averaging word embeddings to form sentence-level representations—a simple but effective baseline. **Attention** generalizes this idea by learning which words matter more in context, assigning dynamic weights instead of treating each word equally. In this sense, **attention can be thought of as a learned, weighted average**.\n",
        "\n",
        "This is our gateway into more advanced techniques. In the next class, we’ll study **self-attention**, the backbone of modern architectures like the Transformer. And if time permits, we may even explore the **full Transformer** model in our final class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kzosqJ1czsY9"
      },
      "id": "kzosqJ1czsY9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yelp Reviews Polarity Dataset  \n",
        "--------------\n",
        "\n",
        "This dataset contains **over 560k full-text reviews** from Yelp, labeled for **binary sentiment**:  \n",
        "- **positive** (5-star reviews)  \n",
        "- **negative** (1-star reviews)  \n",
        "\n",
        "We will not use the full dataset, because it cannot be handled by Colab RAM.\n",
        "\n",
        "Each example is a **real user-generated review**, typically 2–5 sentences long, capturing clear and direct sentiment in natural language.  \n",
        "There are no ambiguous or neutral labels, making this dataset ideal for training and evaluating **binary sentiment classifiers**.\n",
        "\n",
        "The dataset was curated and released as part of the **FastText** and **Text Classification Benchmarks** by researchers at Facebook AI. It is widely used for benchmarking sentiment models in both academia and industry.\n"
      ],
      "metadata": {
        "id": "G7rPtPD1zvCV"
      },
      "id": "G7rPtPD1zvCV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 1. DATA GENERATION  (odd‑one‑out, fixed colour)\n",
        "# ---------------------------------------------\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np, random, torch, os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "IMAGE_SIZE   = 64\n",
        "GRID_SIZE    = 8\n",
        "PATCH        = IMAGE_SIZE // GRID_SIZE          # 8 px\n",
        "RADIUS       = PATCH // 3                       # shape size\n",
        "DATA_ROOT    = \"odd_one_out_imgs\"\n",
        "N_IMAGES     = 10_000                           # total samples\n",
        "TRAIN_FRAC   = 0.8\n",
        "VAL_FRAC     = 0.1\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "SHAPES = (\"circle\", \"square\", \"triangle\")       # all same colour (blue)\n",
        "\n",
        "def make_image(idx):\n",
        "    base_shape = random.choice(SHAPES)\n",
        "    odd_shape  = random.choice([s for s in SHAPES if s != base_shape])\n",
        "    odd_row, odd_col = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)\n",
        "\n",
        "    img = Image.new(\"RGB\", (IMAGE_SIZE, IMAGE_SIZE), \"white\")\n",
        "    dr  = ImageDraw.Draw(img)\n",
        "\n",
        "    for r in range(GRID_SIZE):\n",
        "        for c in range(GRID_SIZE):\n",
        "            x0 = c*PATCH + PATCH//4\n",
        "            y0 = r*PATCH + PATCH//4\n",
        "            x1 = (c+1)*PATCH - PATCH//4\n",
        "            y1 = (r+1)*PATCH - PATCH//4\n",
        "            shape = odd_shape if (r,c)==(odd_row,odd_col) else base_shape\n",
        "            if shape==\"circle\":\n",
        "                dr.ellipse([x0,y0,x1,y1], fill=\"blue\")\n",
        "            elif shape==\"square\":\n",
        "                dr.rectangle([x0,y0,x1,y1], fill=\"blue\")\n",
        "            else: # upright triangle\n",
        "                dr.polygon([(x0,y1),(x1,y1),((x0+x1)//2,y0)], fill=\"blue\")\n",
        "\n",
        "    path = os.path.join(DATA_ROOT, f\"img_{idx:05d}.png\")\n",
        "    img.save(path)\n",
        "    label = odd_row * GRID_SIZE + odd_col       # integer 0‑63\n",
        "    return path, label\n",
        "\n",
        "records = [make_image(i) for i in range(N_IMAGES)]\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. DATASET  →  train / val / test loaders\n",
        "# ---------------------------------------------\n",
        "class OddOneDataset(Dataset):\n",
        "    def __init__(self, items, transform=None):\n",
        "        self.items = items\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.items[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        else: img = torch.tensor(np.array(img), dtype=torch.float32).permute(2,0,1)/255.\n",
        "        return img, label\n",
        "\n",
        "train_items, test_items = train_test_split(records, test_size=1-TRAIN_FRAC, random_state=42)\n",
        "val_items , test_items = train_test_split(test_items, test_size=(1-TRAIN_FRAC-VAL_FRAC)/(1-TRAIN_FRAC), random_state=42)\n",
        "\n",
        "BATCH = 256\n",
        "train_loader = DataLoader(OddOneDataset(train_items), batch_size=BATCH, shuffle=True)\n",
        "val_loader   = DataLoader(OddOneDataset(val_items)  , batch_size=BATCH)\n",
        "test_loader  = DataLoader(OddOneDataset(test_items) , batch_size=BATCH)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3. CNN stem  (5×5→9 → pool → 5×5→16 → pool)\n",
        "# ---------------------------------------------\n",
        "import torch.nn as nn\n",
        "class CNNStem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3,  9, kernel_size=5),   # 64→60\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                   # 60→30\n",
        "            nn.Conv2d(9, 16, kernel_size=5),   # 30→26\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)                    # 26→13\n",
        "        )\n",
        "    def forward(self, x):                      # (B,3,64,64)\n",
        "        return self.net(x)                     # (B,16,13,13)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4. Positional encoding for 13×13=169 tokens\n",
        "# ---------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pos_cnt = feature_cnt = 16\n",
        "def sinusoid_positions(max_len=169, dim=pos_cnt, B=1000):\n",
        "    pos = torch.arange(max_len, device=device).float().unsqueeze(1)\n",
        "    i = torch.arange(dim,  device=device).float().unsqueeze(0)\n",
        "    angle = pos / (B ** (2*(i//2)/dim))\n",
        "    S = torch.zeros(max_len, dim, device=device)\n",
        "    S[:,0::2] = torch.sin(angle[:,0::2])\n",
        "    S[:,1::2] = torch.cos(angle[:,1::2])\n",
        "    return S\n",
        "S = sinusoid_positions()                       # (169,16)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5. Single‑head Self‑Attention module (as given)\n",
        "# ---------------------------------------------\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d=16, k=8):\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(d, k)\n",
        "        self.K = nn.Linear(d, k)\n",
        "        self.V = nn.Linear(d, d)\n",
        "        self.attention = None\n",
        "        self.scale = k ** 0.5\n",
        "    def forward(self, x):                      # (B,N,d)\n",
        "        Q, K, V = self.Q(x), self.K(x), self.V(x)\n",
        "        energies = (Q @ K.transpose(-2,-1)) / self.scale\n",
        "        self.attention = energies.softmax(dim=-1)\n",
        "        return self.attention @ V              # (B,N,d)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 6. Full network: CNN stem → flatten tokens → add PE → attention → classifier\n",
        "# ---------------------------------------------\n",
        "class NetOddOne(nn.Module):\n",
        "    def __init__(self, grid=GRID_SIZE, d=16, k=8):\n",
        "        super().__init__()\n",
        "        self.stem = CNNStem()\n",
        "        self.att  = SelfAttention(d, k)\n",
        "        self.classifier = nn.Linear(d, grid*grid)   # 169 tokens → logits 64\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        feat = self.stem(x)                         # (B,16,13,13)\n",
        "        tokens = feat.permute(0,2,3,1).reshape(B,-1,feature_cnt)  # (B,169,16)\n",
        "        tokens = tokens + S.unsqueeze(0)            # add PE\n",
        "        out = self.att(tokens)                      # (B,169,16)\n",
        "        pooled = out.mean(dim=1)                    # global average\n",
        "        return self.classifier(pooled)              # (B,64)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 7. Training loop (simple)\n",
        "# ---------------------------------------------\n",
        "model = NetOddOne().to(device)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train() if train else model.eval()\n",
        "    tot_loss, tot_correct, tot = 0,0,0\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for x,y in loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "            if train:\n",
        "                opt.zero_grad(); loss.backward(); opt.step()\n",
        "            tot_loss += loss.item()*y.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            tot_correct += (preds==y).sum().item()\n",
        "            tot += y.size(0)\n",
        "    return tot_loss/tot, tot_correct/tot\n",
        "\n",
        "for epoch in range(10):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
        "    val_loss, val_acc = run_epoch(val_loader, False)\n",
        "    print(f\"epoch {epoch:02d} | train {tr_acc:.3f} | val {val_acc:.3f}\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 8. Final test accuracy\n",
        "# ---------------------------------------------\n",
        "test_loss, test_acc = run_epoch(test_loader, False)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "RCp_L4T1t3IC",
        "outputId": "12df4f64-c12d-4c09-8dd1-f36f9cecd358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RCp_L4T1t3IC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 | train 0.014 | val 0.016\n",
            "epoch 01 | train 0.016 | val 0.016\n",
            "epoch 02 | train 0.017 | val 0.018\n",
            "epoch 03 | train 0.019 | val 0.015\n",
            "epoch 04 | train 0.019 | val 0.015\n",
            "epoch 05 | train 0.019 | val 0.015\n",
            "epoch 06 | train 0.019 | val 0.015\n",
            "epoch 07 | train 0.019 | val 0.015\n",
            "epoch 08 | train 0.019 | val 0.015\n",
            "epoch 09 | train 0.021 | val 0.020\n",
            "Test accuracy: 0.021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some examples"
      ],
      "metadata": {
        "id": "oH3nWRZnNAxB"
      },
      "id": "oH3nWRZnNAxB"
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"[{label_map[train_labels[i]]}] {train_texts[i]}\\n\")"
      ],
      "metadata": {
        "id": "2swC8sM0ND49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "b02ae5c5-609f-4efe-dcd8-ed4db21bc369"
      },
      "id": "2swC8sM0ND49",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c9fa68f3b164>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{label_map[train_labels[i]]}] {train_texts[i]}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "-------------------\n",
        "\n",
        "To feed text into a neural network, we need to represent words in a \"neural-network-ish\" way — that is, as numbers. The standard approach is to use a tokenizer, often from a pretrained model. However, since we plan to experiment with our own attention modules later on, **we’ll avoid using any pretrained tokenizer**.\n",
        "\n",
        "Instead, we’ll go with a simple, word-based tokenization. As part of this, we’ll clean the text by removing any non-standard HTML tags, digits, extra whitespace, and punctuation. We’ll also convert all words to lowercase to ensure consistency."
      ],
      "metadata": {
        "id": "T07SvDTYV6nO"
      },
      "id": "T07SvDTYV6nO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Tokens: `<PAD>` and `<UNK>`\n",
        "\n",
        "In our text preprocessing pipeline, we convert each word to a number using a vocabulary. Two special tokens help us handle padding and unknown words.\n"
      ],
      "metadata": {
        "id": "IyT2bcscZu4P"
      },
      "id": "IyT2bcscZu4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### `<PAD>` — Padding Token\n",
        "\n",
        "- Represents empty slots when we need all input sequences to be the same length.\n",
        "- Assigned index `0`.\n",
        "- Used so that batches of sentences can be processed together by the model.\n",
        "\n",
        "*For example:*\n",
        "\n",
        "Original: `[17, 5, 23]`  \n",
        "Padded:   `[17, 5, 23, 0, 0]` (for a fixed length of 5)"
      ],
      "metadata": {
        "id": "Ha9i37gvaBUn"
      },
      "id": "Ha9i37gvaBUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### `<UNK>` — Unknown Token\n",
        "\n",
        "- Represents any word that is **not in the vocabulary**.\n",
        "- Assigned index `1`.\n",
        "- Occurs when:\n",
        "  1. A word was **too rare in the training data** (appeared only once and was excluded from the vocabulary).\n",
        "  2. A word appears **only in validation or test data**.\n",
        "\n",
        "> In our setup, we **excluded all words that appear only once** in the training set.  \n",
        "> So even in the training data, some tokens are replaced with `<UNK>`.  \n",
        "> These are called **rare unknowns** — they help the model learn how to handle unusual or unfamiliar words.\n",
        "\n",
        "\n",
        "By including `<UNK>` during training, we teach the model how to deal with unseen or rare words at test time — which is **crucial for generalization**.\n"
      ],
      "metadata": {
        "id": "3Rymjwd6ZsoK"
      },
      "id": "3Rymjwd6ZsoK"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.split()\n",
        "\n",
        "def build_vocab(token_lists, min_freq=2):\n",
        "    counter = collections.Counter(token for tokens in token_lists for token in tokens)\n",
        "    vocab = {\n",
        "        token: idx + 2  # reserve 0: <PAD>, 1: <UNK>\n",
        "        for idx, (token, count) in enumerate(counter.items())\n",
        "        if count >= min_freq\n",
        "    }\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab\n",
        "\n",
        "def tokens_to_ids(tokens, vocab):\n",
        "    return [vocab.get(tok, vocab['<UNK>']) for tok in tokens]   #for unknown tokens return vocab['<UNK>'] (which is == 1)\n",
        "\n",
        "def pad(seq, max_len=128, pad_value=0):\n",
        "    return seq + [pad_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "\n",
        "def process_texts(texts, vocab, max_len=128):\n",
        "    return [pad(tokens_to_ids(tokenize(text), vocab), max_len) for text in texts]\n",
        "\n",
        "\n",
        "# Tokenize training set and build vocab\n",
        "train_tokens = [tokenize(t) for t in train_texts]\n",
        "vocab = build_vocab(train_tokens)\n",
        "\n",
        "# Process splits into padded input_ids\n",
        "train_ids = process_texts(train_texts, vocab, MAX_LEN)\n",
        "val_ids   = process_texts(val_texts, vocab, MAX_LEN)\n",
        "test_ids  = process_texts(test_texts, vocab, MAX_LEN)\n",
        "\n",
        "# Print 5 real examples: raw text, tokenized, and input IDs\n",
        "shown = 0\n",
        "for i in range(len(train_texts)):\n",
        "    if 1 in train_ids[i][:5]:  # 1 is <UNK>\n",
        "        print(f\"Original:   {train_texts[i]}\")\n",
        "        print(f\"Tokenized:  {train_tokens[i]}\")\n",
        "        print(f\"Input IDs:  {train_ids[i]}\\n\")\n",
        "        shown += 1\n",
        "        if shown >= 5:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "bNJLBhifYFhO"
      },
      "id": "bNJLBhifYFhO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notice!\n",
        "\n",
        "Notice that the `<UNK>` token (coded as 1) is visible in the rows above. Also, there is an abundance of `<PAD>` tokens (coded as 0)."
      ],
      "metadata": {
        "id": "cSDc_W9S89ZU"
      },
      "id": "cSDc_W9S89ZU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data Loaders\n",
        "--------------------\n",
        "\n",
        "We must transform the pandas dataframe to the dataset - it will, among other things, separate input data and labels and then wrap it in a dataloder."
      ],
      "metadata": {
        "id": "XDLaOuwegNXu"
      },
      "id": "XDLaOuwegNXu"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Convert all to tensors\n",
        "def to_loader(input_ids, labels, batch_size=1024, shuffle=False):\n",
        "    x_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
        "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_tensor, y_tensor), batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = to_loader(train_ids, train_labels, BATCH_SIZE, shuffle=True)\n",
        "val_loader   = to_loader(val_ids, val_labels, BATCH_SIZE)\n",
        "test_loader  = to_loader(test_ids, test_labels, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "432ev7Jxg9Fw"
      },
      "id": "432ev7Jxg9Fw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop as We Already Got to Know It Well\n",
        "----------------"
      ],
      "metadata": {
        "id": "bOD1hweknD7z"
      },
      "id": "bOD1hweknD7z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop Function"
      ],
      "metadata": {
        "id": "7kZDRRGS-yMI"
      },
      "id": "7kZDRRGS-yMI"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "def train_model(net, train_loader, val_loader, epochs=1000, lr=0.001, log_every=10):\n",
        "    print(f\"Working on {device}\")\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_inputs, batch_labels in train_loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(device).float()  # shape: (batch_size)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = net(batch_inputs)  # logits shape: (batch_size, 1)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            train_correct += (preds == batch_labels).sum().item()\n",
        "            train_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss_sum / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # === Validation ===\n",
        "        net.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_labels in val_loader:\n",
        "                batch_inputs = batch_inputs.to(device)\n",
        "                batch_labels = batch_labels.to(device).float()\n",
        "\n",
        "                logits = net(batch_inputs)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "\n",
        "                val_loss_sum += loss.item() * batch_inputs.size(0)\n",
        "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "                val_correct += (preds == batch_labels).sum().item()\n",
        "                val_total += batch_inputs.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss_sum / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        if epoch % log_every == 0:\n",
        "            print(f\"Epoch {epoch:03d} | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history"
      ],
      "metadata": {
        "id": "HCA6aXtlnH6J"
      },
      "id": "HCA6aXtlnH6J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Function"
      ],
      "metadata": {
        "id": "-P8_EMUj-tnp"
      },
      "id": "-P8_EMUj-tnp"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(train_loss, val_loss, train_acc, val_acc):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    axs[0].plot(train_loss, label=\"Train Loss\", color='blue')\n",
        "    axs[0].plot(val_loss, label=\"Val Loss\", color='orange')\n",
        "    axs[0].set_title(\"Loss per Epoch\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Average Loss\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    axs[1].plot(train_acc, label=\"Train Accuracy\", color='green')\n",
        "    axs[1].plot(val_acc, label=\"Val Accuracy\", color='red')\n",
        "    axs[1].set_title(\"Accuracy per Epoch\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mDuVP9J6zl0T"
      },
      "id": "mDuVP9J6zl0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple 1 Feature Aproach\n",
        "------------------\n",
        "\n",
        "Before we proceed with a multi-feature network, let's try to go simpler, for a moment. Let's consider a single feature."
      ],
      "metadata": {
        "id": "_GUYSKp3HlRK"
      },
      "id": "_GUYSKp3HlRK"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 1\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #batch, words\n",
        "        features = self.embedding(index)             #batch, words, features\n",
        "        features = features.mean(-2)                 #batch, features\n",
        "        classifications = self.classifier(features)  #batch, 1\n",
        "        logits = classifications.squeeze(-1)         #batch\n",
        "        return logits\n",
        "\n",
        "net_1 = Net_1()\n",
        "\n",
        "# Execute training\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_1, train_loader, val_loader, epochs = 200)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "mNwmWLEuHrOD"
      },
      "id": "mNwmWLEuHrOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Polarity\n",
        "\n",
        "Since our embeddings have only 1 feature (`feature_cnt = 1`), each word is embedded to a scalar. We can interpret this scalar as a kind of sentiment polarity, especially since our model is trained for sentiment classification."
      ],
      "metadata": {
        "id": "mlg7DIdoXA_Y"
      },
      "id": "mlg7DIdoXA_Y"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the embedding weights as a NumPy array\n",
        "embedding_weights = net_1.embedding.weight.detach().cpu().numpy().squeeze()  # shape: (vocab_len,)\n",
        "\n",
        "# Reverse vocab dictionary to map indices back to words\n",
        "id2token = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# Skip <PAD> and <UNK> tokens (indices 0 and 1)\n",
        "valid_indices = np.array([idx for idx in range(2, len(embedding_weights)) if idx in id2token])\n",
        "valid_embeddings = embedding_weights[valid_indices]\n",
        "\n",
        "# Sort and select indices\n",
        "sorted_pos = np.argsort(-valid_embeddings)\n",
        "sorted_neg = np.argsort(valid_embeddings)\n",
        "sorted_neutral = np.argsort(np.abs(valid_embeddings))\n",
        "\n",
        "top_pos_indices = valid_indices[sorted_pos[:20]]\n",
        "top_neg_indices = valid_indices[sorted_neg[:20]]\n",
        "top_neutral_indices = valid_indices[sorted_neutral[:20]]\n",
        "\n",
        "# Print words and corresponding embedding values\n",
        "def print_words_with_embeddings(indices, title):\n",
        "    print(f\"\\n{title}\")\n",
        "    for idx in indices:\n",
        "        word = id2token[int(idx)]\n",
        "        value = embedding_weights[int(idx)]\n",
        "        print(f\"{word:15} -> {value:.4f}\")\n",
        "\n",
        "print_words_with_embeddings(top_pos_indices, \"Top 20 most positive words:\")\n",
        "print_words_with_embeddings(top_neg_indices, \"Top 20 most negative words:\")\n",
        "print_words_with_embeddings(top_neutral_indices, \"Top 20 most neutral words:\")\n"
      ],
      "metadata": {
        "id": "zPijTrZEXvM4"
      },
      "id": "zPijTrZEXvM4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificator\n",
        "\n",
        "It is interesting to see how the polar sentiment gets translated into the two  class values. Let's see:"
      ],
      "metadata": {
        "id": "ayZtmbdcvPyP"
      },
      "id": "ayZtmbdcvPyP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract weights and bias from the classifier layer\n",
        "classifier_weight = net_1.classifier.weight.detach().cpu().numpy()\n",
        "classifier_bias = net_1.classifier.bias.detach().cpu().numpy()\n",
        "\n",
        "print(\"Classifier weights (shape: {}):\".format(classifier_weight.shape))\n",
        "print(classifier_weight)\n",
        "\n",
        "print(\"\\nClassifier bias (shape: {}):\".format(classifier_bias.shape))\n",
        "print(classifier_bias)\n",
        "\n",
        "print(\"Recall our coding: \")\n",
        "print(label_map)\n"
      ],
      "metadata": {
        "id": "0Ra3BaKGv7ib"
      },
      "id": "0Ra3BaKGv7ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand-picked test sentences\n",
        "texts = [\n",
        "    \"I love this\",\n",
        "    \"I like this\",\n",
        "    \"I do not like this\",\n",
        "    \"I don't like this\",\n",
        "    \"This is terrible\",\n",
        "    \"Thank you so much!\",\n",
        "    \"I hate this\",\n",
        "    \"I don't hate this\",\n",
        "    \"This sucks!\"\n",
        "]\n",
        "\n",
        "# Convert texts to input_ids using your tokenizer\n",
        "input_ids = process_texts(texts, vocab)\n",
        "input_tensor = torch.tensor(input_ids).to(next(net_1.parameters()).device)\n",
        "\n",
        "# Predict with trained model\n",
        "net_1.eval()\n",
        "with torch.no_grad():\n",
        "    logits = net_1(input_tensor).squeeze()\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Print results\n",
        "for text, prob in zip(texts, probs):\n",
        "    print(f\"{text:30} -> predicted probability of POSITIVE: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "00X0GbtFELx7"
      },
      "id": "00X0GbtFELx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Artificial (Concatenated) Sentences\n",
        "-----------------\n",
        "\n",
        "It will make the dataset diffinitely more demanding and also more suited for attention that we intend to add later.\n",
        "\n",
        "Every original sentence appears as the second part of a mixed sentence, so every label is retained. Next, a second randomly selected sentence is prepended — that is, added to the beginning of the original sentence. As an effect, we generate as many augmented samples as there are originals. The number of samples and the label distribution is retained.\n",
        "\n",
        "The classifier must learn to focus on the main sentence and ignore the distractor that is added. This provides a clear, controlled task that makes attention valuable:\n",
        " - **With attention**: the model can learn to give weight to the main part. It must also learn to disregard `<PAD>` tokens that follow.\n",
        " - **Without attention**: a mean-pool model may be diluted by the additional sentence."
      ],
      "metadata": {
        "id": "wj10SuUbZDmM"
      },
      "id": "wj10SuUbZDmM"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "MAX_LEN = 256  # changing max len to accomodate longer sequences, also, longer sequences are harder for the mean network\n",
        "\n",
        "def pair_every_sentence_with_random(texts, labels, seed=42):\n",
        "    \"\"\"\n",
        "    For each sentence, append a randomly selected second sentence.\n",
        "    The label of the first is preserved.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    n = len(texts)\n",
        "\n",
        "    augmented_texts = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for i in range(n):\n",
        "        first = texts[i]\n",
        "        label = labels[i]\n",
        "\n",
        "        second = texts[random.randint(0, n - 1)]\n",
        "\n",
        "        combined = second + \" \" + first # we prepend the sentence with a random one, now\n",
        "\n",
        "        augmented_texts.append(combined)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "    return texts + augmented_texts, labels + augmented_labels\n",
        "\n",
        "\n",
        "train_texts, train_labels = pair_every_sentence_with_random(train_texts, train_labels)\n",
        "val_texts, val_labels     = pair_every_sentence_with_random(val_texts,   val_labels)\n",
        "test_texts, test_labels   = pair_every_sentence_with_random(test_texts,  test_labels)\n",
        "\n",
        "# Reprocess splits into padded input_ids\n",
        "train_ids = process_texts(train_texts, vocab, MAX_LEN)\n",
        "val_ids   = process_texts(val_texts, vocab, MAX_LEN)\n",
        "test_ids  = process_texts(test_texts, vocab, MAX_LEN)\n",
        "\n",
        "# Recalculate data loaders\n",
        "train_loader = to_loader(train_ids, train_labels, BATCH_SIZE, shuffle=True)\n",
        "val_loader   = to_loader(val_ids, val_labels, BATCH_SIZE)\n",
        "test_loader  = to_loader(test_ids, test_labels, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "p_XV_kiyZOEf"
      },
      "id": "p_XV_kiyZOEf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Features, Now\n",
        "---------------"
      ],
      "metadata": {
        "id": "Gn__dzYW2PWA"
      },
      "id": "Gn__dzYW2PWA"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_F(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #batch, words\n",
        "        features = self.embedding(index)             #batch, words, features\n",
        "        features = features.mean(-2)                 #batch, features\n",
        "        classifications = self.classifier(features)  #batch, 1\n",
        "        logits = classifications.squeeze(-1)         #batch\n",
        "        return logits\n",
        "\n",
        "net_f=Net_F()\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_f, train_loader, val_loader, epochs=50)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "fSsb18Pf2hZP"
      },
      "id": "fSsb18Pf2hZP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Towards Attention!\n",
        "---------------------------\n"
      ],
      "metadata": {
        "id": "cnwL4ci4aRMK"
      },
      "id": "cnwL4ci4aRMK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Entry Point\n",
        "\n",
        "The entry point is our model with averaged multiple features:"
      ],
      "metadata": {
        "id": "ZeX-Tnlra-pY"
      },
      "id": "ZeX-Tnlra-pY"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_F(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #SIZE: batch, words\n",
        "        features = self.embedding(index)             #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where AVERAGING takes place\n",
        "        features = features.mean(-2)                 #SIZE: batch, features\n",
        "\n",
        "\n",
        "        classifications = self.classifier(features)  #SIZE: batch, 1\n",
        "        logits = classifications.squeeze(-1)         #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "diAmuIhIbHOQ"
      },
      "id": "diAmuIhIbHOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Less Explicit Averaging"
      ],
      "metadata": {
        "id": "H-g5Q7KxbWzG"
      },
      "id": "H-g5Q7KxbWzG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is equivalent to taking the `mean()` over words:"
      ],
      "metadata": {
        "id": "KLDhT9JCd9gh"
      },
      "id": "KLDhT9JCd9gh"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        weights = torch.ones((batch, 1, words)) / words   # create EQUAL WEIGHT tensor summing to 1.0 ( words x (1/words) )\n",
        "                                                         #SIZE: batch, 1, words\n",
        "        features = weights @ features                    #SIZE: batch, 1, features\n",
        "\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "epoXI3SZbcHg"
      },
      "id": "epoXI3SZbcHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let us refactor this, taking the weighted average part into a separate `Attention` module:"
      ],
      "metadata": {
        "id": "OFcB29hteVru"
      },
      "id": "OFcB29hteVru"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, features):                         #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        weights = torch.ones((batch, 1, words)) / words   # create EQUAL WEIGHT tensor summing to 1.0 ( words x (1/words) )\n",
        "                                                         #SIZE: batch, 1, words\n",
        "        features = weights @ features                    #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "8FzeUuJ-ehns"
      },
      "id": "8FzeUuJ-ehns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that we did up until that point was to rewrite taking the `mean()` into a separate `Attention` which calculates the weighted averager with equal weights over weights."
      ],
      "metadata": {
        "id": "fKuofPqsfWxe"
      },
      "id": "fKuofPqsfWxe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Notion of Energy\n",
        "\n",
        "Another useful concept is that of the energy. The energy equal to 0 uniformly for all words translates (with the use of `softmax`) into the equal weights, so the below version is still equivalent to what we already had (but, arguably, it looks much more complex):"
      ],
      "metadata": {
        "id": "3cFkehdKf1vE"
      },
      "id": "3cFkehdKf1vE"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = torch.zeros((batch, 1, words))       #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_Towards_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with EQUAL WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits"
      ],
      "metadata": {
        "id": "IVkMyyYjgN6p"
      },
      "id": "IVkMyyYjgN6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Something New - Learned Energies"
      ],
      "metadata": {
        "id": "8Q4efFtch23v"
      },
      "id": "8Q4efFtch23v"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.energy = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = self.energy(features)                #SIZE: batch, words, 1\n",
        "        energies = energies.transpose(-2, -1)           #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, feature_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index)                 #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with LEARNED WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits\n",
        "\n",
        "net_att = Net_Attention()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att, train_loader, val_loader, epochs = 50)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "8VLpkoPbh9LG"
      },
      "id": "8VLpkoPbh9LG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n",
        "-----------------------"
      ],
      "metadata": {
        "id": "GnWwejPa68Fi"
      },
      "id": "GnWwejPa68Fi"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "pos_cnt = 16\n",
        "positional_encoding_B = 1000\n",
        "\n",
        "def sinusoid_positions(max_len = MAX_LEN, dim = pos_cnt):\n",
        "    pos = torch.arange(max_len, device=device).float().unsqueeze(1)\n",
        "    i   = torch.arange(dim, device=device).float().unsqueeze(0)\n",
        "    angle = pos / (positional_encoding_B ** (2 * (i//2) / dim))\n",
        "    S = torch.zeros(max_len, dim, device=device)\n",
        "    S[:, 0::2] = torch.sin(angle[:, 0::2])\n",
        "    S[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "    return S          #  SIZE: words, features  (constant matrix throuoght all computations)\n",
        "\n"
      ],
      "metadata": {
        "id": "oz5NJuqA7qoT"
      },
      "id": "oz5NJuqA7qoT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Each position in the sequence gets a vector that is added (or concatenated) to the word embedding. The values in that vector are defined deterministically using sine and cosine waves of varying frequencies.\n",
        "\n",
        "For a given position $pos$ and dimension $i$ (where $i$ goes from $0$ to $d_{\\text{model}} - 1$):\n",
        "\n",
        "$$\n",
        "\\text{PE}_{pos, i} =\n",
        "\\begin{cases}\n",
        "\\sin\\left(\\frac{pos}{B^{i / d_{\\text{model}}}}\\right), & \\text{if } i \\text{ is even} \\\\\n",
        "\\cos\\left(\\frac{pos}{B^{(i - 1) / d_{\\text{model}}}}\\right), & \\text{if } i \\text{ is odd}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "with $B=10,000$, typically.\n",
        "\n",
        "\n",
        "\n",
        "**Why use sine and cosine?**\n",
        "\n",
        "- **Smoothness**: adjacent positions have similar encodings — helpful for capturing local context.\n",
        "- **Distance-preserving**: differences between position encodings reflect relative distances, which helps the attention mechanism.\n",
        "- **No training needed**: the encoding is deterministic, so it generalizes to sequences longer than those seen during training.\n"
      ],
      "metadata": {
        "id": "drCE7Wfv724t"
      },
      "id": "drCE7Wfv724t"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Create sinusoidal matrix S\n",
        "S = sinusoid_positions() # size: words, features\n",
        "\n",
        "# Separate sine (even) and cosine (odd) components\n",
        "sine = S[:, ::2]\n",
        "cosine = S[:, 1::2]\n",
        "gap = torch.full((S.size(0), 1), float('nan')).to(device)  # vertical separator\n",
        "\n",
        "# Concatenate: [sine | gap | cosine]\n",
        "S_sep = torch.cat([sine, gap, cosine], dim=1)\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(S_sep.cpu().numpy(), cmap=\"coolwarm\", cbar=True, xticklabels=2, yticklabels=4)\n",
        "plt.title(\"Sinusoidal Positional Encodings (Sine | Cosine)\")\n",
        "plt.xlabel(\"Embedding dimension (sine for even | gap | cosine for odd)\")\n",
        "plt.ylabel(\"Token position\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(S.cpu().numpy(), cmap=\"coolwarm\", cbar=True, xticklabels=2, yticklabels=4)\n",
        "plt.title(\"Sinusoidal Positional Encodings\")\n",
        "plt.xlabel(\"Embedding dimension\")\n",
        "plt.ylabel(\"Token position\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "R3PFWarM7O7C"
      },
      "id": "R3PFWarM7O7C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cnt = 16          # word embeddings\n",
        "pos_cnt = 16            # pos embeddings\n",
        "feature_cnt = 16\n",
        "\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.energy = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, features):                        #SIZE: batch, words, features\n",
        "        batch = features.size(0)          # get the batch dimension\n",
        "        words = features.size(1)          # get the words dimension\n",
        "        energies = self.energy(features)                #SIZE: batch, words, 1\n",
        "        energies = energies.transpose(-2, -1)           #SIZE: batch, 1, words\n",
        "        weights = F.softmax(energies, -1)               #SIZE: batch, 1, words\n",
        "        features = weights @ features                   #SIZE: batch, 1, features\n",
        "        return features\n",
        "\n",
        "class Net_Attention_And_PE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, embed_cnt)\n",
        "        self.attention = Attention()\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                            #SIZE: batch, words\n",
        "        features = self.embedding(index) + S             #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where WHEIGHTED AVERAGING with LEARNED WEIGHTS takes place\n",
        "        features = self.attention(features)              #SIZE: batch, 1, features\n",
        "\n",
        "        classifications = self.classifier(features)      #SIZE: batch, 1, 1\n",
        "        logits = classifications.squeeze(-1).squeeze(-1) #SIZE: batch\n",
        "        return logits\n",
        "\n",
        "net_att_pe = Net_Attention_And_PE()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_att_pe, train_loader, val_loader, epochs = 20)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "UyIkrhVH8FPD"
      },
      "id": "UyIkrhVH8FPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cnt = 16          # word embeddings\n",
        "pos_cnt = 16             # pos embeddings\n",
        "feature_cnt = 16\n",
        "vocab_len = max(vocab.values()) + 1   # maximal value (index) of a token\n",
        "\n",
        "class Net_PE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_len, embed_cnt)\n",
        "        self.classifier = torch.nn.Linear(feature_cnt, 1)\n",
        "    def forward(self, index):                        #SIZE: batch, words\n",
        "        features = self.embedding(index) + S         #SIZE: batch, words, features\n",
        "\n",
        "        ### This is where AVERAGING takes place\n",
        "        features = features.mean(-2)                 #SIZE: batch, features\n",
        "\n",
        "        classifications = self.classifier(features)  #SIZE: batch, 1\n",
        "        logits = classifications.squeeze(-1)         #SIZE: batch\n",
        "        return logits\n",
        "\n",
        "net_pe = Net_PE()\n",
        "\n",
        "# Execute training again\n",
        "train_loss, val_loss, train_acc, val_acc = train_model(net_pe, train_loader, val_loader, epochs = 50)\n",
        "plot_training_curves(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "qd-X2nqz9K96"
      },
      "id": "qd-X2nqz9K96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "**Attention-based models outperform non-attention models**, even when positional encoding is included in the latter.\n",
        "\n",
        "But does that mean a non-attention model is **incapable** of learning which words and positions are important and how they relate to sentiment polarity?\n",
        "\n",
        "**Not necessarily.**  \n",
        "Especially with more depth, a regular network could still learn to associate **word importance** with **sentiment signals**. It can, in principle, learn both at once.\n",
        "\n",
        "However, the advantage of attention is that it **explicitly separates** these two aspects:\n",
        "- the **informational content** of the input, and  \n",
        "- the **relative importance** of different parts.\n",
        "\n",
        "This division simplifies the learning process and makes the model's behavior more interpretable.\n",
        "\n",
        "There are other examples, where embedding known mathematical structure\n",
        "makes learning easier, faster, and more efficient.  \n",
        "**Don’t let the network relearn what you can hard-code for free.**\n",
        "\n",
        "The two examples will follow:\n",
        "- output range constraint\n",
        "- a Green function for PINNs"
      ],
      "metadata": {
        "id": "oyDQ9UZ4aZCp"
      },
      "id": "oyDQ9UZ4aZCp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result in (0,1)\n",
        "\n",
        "This is conceptually similar to applying a **sigmoid** to the output of a neural network  \n",
        "if we know the target should lie in $(0, 1)$ — we let the network focus only on the hard part, instead of having it rediscover itself that the result is in (0,1)."
      ],
      "metadata": {
        "id": "tGFhqIrkmozQ"
      },
      "id": "tGFhqIrkmozQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Green function\n",
        "\n",
        "A **Green function** is a fundamental solution used to solve linear differential equations with boundary conditions.  \n",
        "It represents the **response of the system** (e.g., vibrating string, heat rod) to a **unit point source** located at position $\\xi$.\n",
        "\n",
        "Once the Green function $G(x, \\xi)$ is known, the solution to the full problem with an arbitrary source function $f(\\xi)$ can be written as:\n",
        "\n",
        "$$\n",
        "u(x) = \\int_{\\Omega} G(x, \\xi)\\, f(\\xi)\\, d\\xi + \\text{(boundary terms)}.\n",
        "$$\n",
        "\n",
        "This turns the differential equation into an **integral equation**, which is often more stable or efficient to solve numerically.  \n",
        "The Green function $G(x, \\xi)$ is fully determined by the PDE and boundary conditions.  \n",
        "The only unknown is the source term $f(\\xi)$ (or possibly a coefficient field).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VvSHP56gmOuY"
      },
      "id": "VvSHP56gmOuY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How It's Used in a PINN (Physics-Informed Neural Network)\n",
        "\n",
        "PINNs are neural network used to solve differential equations arising in physics.\n",
        "\n",
        "**Give the network the easy part for free.**  \n",
        "If you already know the Green function, you can embed it into the model output:\n",
        "\n",
        "$$\n",
        "u_\\theta(x) = \\int_{\\Omega} G(x, \\xi)\\, N_\\theta(\\xi)\\, d\\xi + \\text{(boundary term)},\n",
        "$$\n",
        "\n",
        "where $N_\\theta$ is a neural network that learns only the **residual source** $f(\\xi)$.  \n",
        "We then train $N_\\theta$ by minimizing the PDE residual.\n",
        "\n",
        "Because the Green function already builds in the correct inverse operator and boundary behavior,  \n",
        "the network no longer wastes capacity trying to learn what we already know."
      ],
      "metadata": {
        "id": "EBtepBfunRXT"
      },
      "id": "EBtepBfunRXT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Weights Exploration\n",
        "\n",
        "\n",
        "\n",
        "Now that our attention-based model is trained, and is superior to non-attention model, let's take a closer look at what it's *actually attending to*. We'll manually construct artificial embeddings containing:\n",
        "\n",
        "- A strongly positive word (`delicious`)\n",
        "- A strongly negative word (`disappointing`)\n",
        "- A neutral word (`delivery`)\n",
        "- A padding token (`<PAD>`)\n",
        "\n",
        "plus the positional encoding for all positions.\n",
        "\n",
        "Each word will be passed through the model along with positional encodings.\n",
        "\n",
        "We'll then extract and chart the raw attention **energies** that the network computes **before softmax** — showing how much each word is prioritized in the weighted average.\n"
      ],
      "metadata": {
        "id": "rtoGDOMtDeoX"
      },
      "id": "rtoGDOMtDeoX"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Words to test\n",
        "probe_words = [\"delicious\", \"disappointing\", \"delivery\", \"<PAD>\"]\n",
        "\n",
        "# Evaluate in no-grad mode\n",
        "net_att_pe.eval()\n",
        "with torch.no_grad():\n",
        "    for word in probe_words:\n",
        "        token_idx = vocab.get(word, vocab[\"<UNK>\"])\n",
        "        token_embed = net_att_pe.embedding(torch.tensor([token_idx]).to(device)) # size: 1, features\n",
        "\n",
        "        energies = []\n",
        "        for pos in range(MAX_LEN):\n",
        "            pos_enc = S[pos]                                          # size: features\n",
        "            full_input = token_embed + pos_enc                        # size: 1, features\n",
        "            energy = net_att_pe.attention.energy(full_input)          # size: 1, 1\n",
        "            energies.append(energy.item())\n",
        "\n",
        "        # Plot the energy across positions\n",
        "        plt.figure(figsize=(8, 3))\n",
        "        plt.plot(range(MAX_LEN), energies)\n",
        "        plt.title(f\"Attention Energy vs. Position — {word}\")\n",
        "        plt.xlabel(\"Position (pos_enc index)\")\n",
        "        plt.ylabel(\"Raw Attention Energy\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "_Y9X9VyMD5ZT"
      },
      "id": "_Y9X9VyMD5ZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things to Note\n",
        "\n",
        "The attention component is linear with respect to its input, so it's no surprise that the energy curves look similar in shape. However, note two important observations:\n",
        "\n",
        "- The attention energy curves **increase** with respect to position — they generally slope upwards. This is in response to the second sentence being more important in terms of label determination.\n",
        "- The **starting and ending values** of the curves differ between tokens:\n",
        "  - They are **positive** for important words (those shaded strongly positive or negative by the classifier).\n",
        "  - They are **mid-valued** for neutral words.\n",
        "  - They are **negative** for the `<PAD>` token — which often appears at the end of the second sentence (i.e., the important segment of the sequence). Despite being in an \"important\" location, the `<PAD>` token contributes nothing useful and **dilutes the average**, so the attention mechanism learns to assign it a **negative energy** to downweight it before softmax normalization."
      ],
      "metadata": {
        "id": "XXVc9BfBcGer"
      },
      "id": "XXVc9BfBcGer"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Is a *queen* really just a *king*, minus a *man*, plus a *woman*?**\n",
        "\n",
        "--------------\n",
        "\n",
        "\n",
        "\n",
        "In class, we dealt with **embeddings** trained for **sentiment classification**. These embeddings are optimized to separate *positive* from *negative* expressions and **do not encode deeper semantic information**.\n",
        "\n",
        "However, in modern natural language processing, there exist other embeddings — such as those from **BERT**, **word2vec**, or **GloVe** — that **do capture semantic structure**. These models are trained on large corpora, and their embeddings often allow for meaningful **vector arithmetic**, like the famous:\n",
        "\n",
        "```\n",
        "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ≈ embedding(\"queen\")\n",
        "```\n",
        "\n",
        "This homework explores **semantic vector relationships** using such pretrained embeddings.\n",
        "\n",
        "## **The Objective**\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "1. Construct semantic classes of word pairs.\n",
        "2. Visualize them using PCA.\n",
        "3. Explore arithmetic operations in embedding space.\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "### 1. **Semantic Pair Classes**\n",
        "\n",
        "- You must gather **at least 10 classes** of semantically related word pairs.\n",
        "- Each class must contain **at least 5 pairs**.\n",
        "- That gives a **minimum total of 100 unique words** (10 classes x 5 pairs x 2 words per pair).\n",
        "\n",
        "Two example classes:\n",
        "\n",
        "**Class 1: Gender**\n",
        "\n",
        "- (king, queen)\n",
        "- (man, woman)\n",
        "- (doctor, nurse)\n",
        "- (prince, princess)\n",
        "- *(you must add one more)*\n",
        "\n",
        "**Class 2: Verb tense (past tense)**\n",
        "\n",
        "- (bring, brought)\n",
        "- (get, got)\n",
        "- (like, liked)\n",
        "- *(you must add two more)*\n",
        "\n",
        "**Your job:**\n",
        "\n",
        "- Invent or search for **at least 10 such classes**, including the examples above.\n",
        "- Each class must be conceptually coherent.\n",
        "- Other examples: singular/plural, country/capital, comparative/superlative, tool/user, job/object, etc.\n",
        "\n",
        "### 2. **Global PCA (Across All Words)**\n",
        "\n",
        "- Use PCA to reduce the **entire set of 100 word embeddings** to 2D, and plot it.\n",
        "- Plot the additional **10 separate charts**, one for each class.\n",
        "  - Each chart should display only the 10 words (5 pairs) of the given class.\n",
        "- Points should be labeled with the words themselves.\n",
        "\n",
        "### 3. **Local PCA (Per Class)**\n",
        "\n",
        "- For each class (10 total), perform PCA **only** on the 10 words of that class.\n",
        "- Plot these class-wise PCA visualizations as separate charts.\n",
        "- Again, points should be labeled with the words.\n",
        "\n",
        "**Total: 21 charts**\n",
        "(1 global plot with 100 words + 10 global-space class plots + 10 local PCA class plots)\n",
        "\n",
        "Charts should be presented in a self-explanatory manner with clear labels.\n",
        "\n",
        "### 4. **Embedding Arithmetic**\n",
        "\n",
        "For each class, choose **one example pair** (e.g., (king, queen)) and perform the operation:\n",
        "\n",
        "```\n",
        "embedding(B) - embedding(A) + embedding(C)\n",
        "```\n",
        "\n",
        "Where A and B form a known pair, and C is another base word.\n",
        "For example:\n",
        "\n",
        "```\n",
        "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
        "```\n",
        "\n",
        "* For each such result vector, find the **5 closest word embeddings** (using cosine similarity or Euclidean distance).\n",
        "* Print the top 5 neighbors **with their distances**.\n",
        "* Do this **once per class** (i.e., 10 times).\n",
        "\n",
        "This will make it possible to verify if\n",
        " ```\n",
        "embedding(\"queen\") ≈ embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
        "```\n",
        "for the *gender*-related class.\n",
        "\n",
        "\n",
        "### 5. **Discussion**\n",
        "\n",
        "* Analyze and interpret your 21 plots.\n",
        "* Discuss whether the vector relationships are preserved.\n",
        "* Does PCA capture semantic differences?\n",
        "* Are the closest words from the arithmetic meaningful?\n",
        "* What kinds of relationships are captured, and what are not?\n",
        "* Are some classes better behaved than others?\n",
        "\n",
        "\n",
        "### 6. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
        "\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "*This homework assignment was inspired by an idea from my master's student **Andrzej Małek**, to whom I would like to express my thanks.*\n",
        "\n"
      ],
      "metadata": {
        "id": "_pm7h5fXtWfY"
      },
      "id": "_pm7h5fXtWfY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can $W_V^{(h)}$ and the per‑head slice of $W_O$ be collapsed?\n",
        "\n",
        "Let  \n",
        "\n",
        "* $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$ (input tokens)  \n",
        "* $W_V^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ (value projection for head $h$)  \n",
        "* $W_O^{(h)} \\in \\mathbb{R}^{d_v \\times d_{\\text{model}}}$ (the block of $W_O$ that multiplies head $h$)\n",
        "\n",
        "---\n",
        "\n",
        "#### Forward path for a single head  \n",
        "\n",
        "1. **Values** $V_h = X\\,W_V^{(h)} \\;\\in \\mathbb{R}^{n \\times d_v}$  \n",
        "2. **Attention mixing** $Z_h = A_h\\,V_h$  \n",
        "3. **Output projection** $Y_h = Z_h\\,W_O^{(h)}$\n",
        "\n",
        "Total head output:  \n",
        "\n",
        "$$\n",
        "Y_h \\;=\\; A_h\\,\\bigl(X\\,W_V^{(h)}\\bigr)\\,W_O^{(h)}\n",
        "          \\;=\\; A_h\\,X\\,\\bigl(W_V^{(h)} W_O^{(h)}\\bigr).\n",
        "$$\n",
        "\n",
        "So **inside the attention block** the two matrices *do* multiply to a single\n",
        "$d_{\\text{model}}\\!\\times\\! d_{\\text{model}}$ matrix  \n",
        "\n",
        "$$\n",
        "\\tilde W^{(h)} \\;=\\; W_V^{(h)} W_O^{(h)}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Why not learn one big matrix per head?\n",
        "\n",
        "| quantity                               | shape | parameters per head |\n",
        "|----------------------------------------|-------|---------------------|\n",
        "| $W_V^{(h)}$                            | $d_{\\text{model}} \\times d_v$ | $d_{\\text{model}}\\,d_v$ |\n",
        "| $W_O^{(h)}$                            | $d_v \\times d_{\\text{model}}$ | $d_{\\text{model}}\\,d_v$ |\n",
        "| **total with two thin matrices**       | —     | $2\\,d_{\\text{model}}\\,d_v$ |\n",
        "| single fat matrix $\\tilde W^{(h)}$     | $d_{\\text{model}} \\times d_{\\text{model}}$ | $d_{\\text{model}}^{\\,2}$ |\n",
        "\n",
        "Because $d_v \\ll d_{\\text{model}}$ (e.g.\\ $d_v=64$, $d_{\\text{model}}=512$),\n",
        "using the **two thin matrices saves parameters**:\n",
        "\n",
        "$$\n",
        "\\frac{2\\,d_{\\text{model}}\\,d_v}{d_{\\text{model}}^{\\,2}}\n",
        "\\;=\\;\n",
        "\\frac{2\\,d_v}{d_{\\text{model}}}\n",
        "\\;\\ll\\;1.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Can we move the product past attention?\n",
        "\n",
        "No.  \n",
        "Even though $W_V^{(h)} W_O^{(h)}$ collapses *inside* the head, the result still\n",
        "sits **to the right of the attention matrix** $A_h$:\n",
        "\n",
        "$$\n",
        "Y_h \\;=\\; A_h\\,X\\,\\tilde W^{(h)},\n",
        "$$\n",
        "\n",
        "and $A_h = \\text{softmax}\\!\\bigl(Q_h K_h^\\top / \\sqrt{d_k}\\bigr)$ depends on $X$.  \n",
        "Therefore $\\tilde W^{(h)}$ **cannot** be pulled left of $A_h$, so the non‑linearity introduced by softmax is preserved.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "\n",
        "* Mathematically, $W_V^{(h)}$ and the head slice $W_O^{(h)}$ do multiply to one matrix inside the head, **but**  \n",
        "  * keeping them separate reduces parameters from $d_{\\text{model}}^{\\,2}$ to $2\\,d_{\\text{model}}\\,d_v$,\n",
        "  * softmax attention still makes the overall mapping non‑linear in $X$.\n",
        "* Hence in practice both $W_V$ and $W_O$ are retained for **parameter efficiency** and to keep the standard multi‑head formulation intact.\n"
      ],
      "metadata": {
        "id": "zdgnAoH2KeOW"
      },
      "id": "zdgnAoH2KeOW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you form  \n",
        "\n",
        "$$\n",
        "Y_h \\;=\\; A_h\\,X\\,\\tilde W^{(h)},\n",
        "\\quad\\text{where}\\;\n",
        "\\tilde W^{(h)} = W_V^{(h)} W_O^{(h)},\n",
        "$$\n",
        "\n",
        "the feed‑forward sub‑layer does\n",
        "\n",
        "$$\n",
        "H \\;=\\; W_2\\,\\text{ReLU}(Y_h\\,W_1+b_1)+b_2\n",
        "      \\;=\\; W_2\\,\\text{ReLU}\\!\\bigl(A_h\\,X\\,\\tilde W^{(h)}\\,W_1 + b_1\\bigr)+b_2 .\n",
        "$$\n",
        "\n",
        "**Yes, $W_1$ multiplies the already‑collapsed $\\tilde W^{(h)}$,\n",
        "so inside the head those two matrices become one larger matrix**\n",
        "\n",
        "$$\n",
        "\\hat W^{(h)} \\;=\\; \\tilde W^{(h)}\\,W_1\n",
        "                \\;\\in\\; \\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{ff}}}.\n",
        "$$\n",
        "\n",
        "But there are still two reasons this is **not a redundant collapse**:\n",
        "\n",
        "| Reason | Explanation |\n",
        "|--------|-------------|\n",
        "| **1&nbsp; Parameter economy** | $\\tilde W^{(h)}$ is only $d_{\\text{model}}\\!\\times\\! d_v$ and $W_1$ is $d_{\\text{model}}\\!\\times\\! d_{\\text{ff}}$ shared **across all heads**.  A single $\\hat W^{(h)}$ would be $d_{\\text{model}}\\!\\times\\! d_{\\text{ff}}$ *per head*—quadratically more parameters. |\n",
        "| **2&nbsp; Non‑linearity and data dependence remain** | The term $A_h$ precedes $\\hat W^{(h)}$.  Because $A_h=\\text{softmax}(Q_hK_h^\\top/\\sqrt{d_k})$ depends on $X$, the full map is still **non‑linear** in the input; you cannot premultiply $W_2$ with $\\hat W^{(h)}$ across the ReLU. |\n",
        "\n",
        "So while \\(W_V^{(h)}\\), \\(W_O^{(h)}\\), and \\(W_1\\) do algebraically merge into one matrix **inside each head**, keeping them separate\n",
        "\n",
        "* saves parameters (thin × thin vs. one fat),\n",
        "* allows weight sharing across heads,\n",
        "* preserves the intended dimension expansion ($d_{\\text{model}}\\!\\rightarrow\\! d_{\\text{ff}}$),\n",
        "* and leaves the overall mapping non‑linear because of the preceding softmax and the intervening ReLU.\n",
        "\n",
        "Therefore \\(W_1\\) is **not superfluous**; it supplies width and non‑linearity after the attention mixing, even though its multiplication can be factored into the per‑head linear product.\n"
      ],
      "metadata": {
        "id": "ik2yDAxQTkEr"
      },
      "id": "ik2yDAxQTkEr"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ge8AWMILtvMh"
      },
      "id": "Ge8AWMILtvMh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}